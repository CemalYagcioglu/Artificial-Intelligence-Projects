{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\", color_codes=True)\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "from glob import glob\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn.feature_selection import SelectFromModel, VarianceThreshold\n",
    "\n",
    "import sys;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'./dengue_features_test.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0f7ae46c6350>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeatures_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./dengue_features_test.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# , index_col=0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfeatures_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./dengue_features_train.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlabels_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./dengue_labels_train.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Cemal/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Cemal/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Cemal/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Cemal/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Cemal/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas/_libs/parsers.c:4209)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas/_libs/parsers.c:8873)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'./dengue_features_test.csv' does not exist"
     ]
    }
   ],
   "source": [
    "features_test = pd.read_csv('./dengue_features_test.csv', parse_dates=True, delimiter=',') # , index_col=0\n",
    "features_train = pd.read_csv('./dengue_features_train.csv', parse_dates=True, delimiter=',')\n",
    "labels_train = pd.read_csv('./dengue_labels_train.csv', parse_dates=True, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop Certain Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#features_train.drop('year', axis=1, inplace=True)\n",
    "#features_train.drop('weekofyear', axis=1, inplace=True)\n",
    "features_train.drop('week_start_date', axis=1, inplace=True)\n",
    "features_train.drop('city', axis=1, inplace=True)\n",
    "\n",
    "labels_train.drop('year', axis=1, inplace=True)\n",
    "labels_train.drop('weekofyear', axis=1, inplace=True)\n",
    "labels_train.drop('city', axis=1, inplace=True)\n",
    "\n",
    "#features_train.drop('year', axis=1, inplace=True)\n",
    "#features_train.drop('weekofyear', axis=1, inplace=True)\n",
    "features_test.drop('week_start_date', axis=1, inplace=True)\n",
    "features_test.drop('city', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_train = (features_train -  features_train.mean()) / features_train.std()\n",
    "features_test = (features_test -  features_test.mean()) / features_test.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Fill in NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_train.fillna(method='bfill', inplace=True)\n",
    "features_test.fillna(method='bfill', inplace=True)\n",
    "labels_train.fillna(method='bfill', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = model_selection.train_test_split(features_train, \n",
    "                                                                  labels_train, test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Columns for DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_columns = tf.contrib.learn.infer_real_valued_columns_from_input(features_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Test Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn import linear_model\n",
    "from sklearn import cross_validation\n",
    "from sklearn import ensemble\n",
    "\n",
    "#imports\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lm = linear_model.LinearRegression()\n",
    "\n",
    "dnn = tf.contrib.learn.DNNRegressor(feature_columns=feature_columns, hidden_units=[20,15], \n",
    "                                    activation_fn=tf.nn.relu6) #, tf_random_seed = 101)\n",
    "\n",
    "xgbr = xgb.XGBRegressor(seed = 101)\n",
    "\n",
    "gbr = ensemble.GradientBoostingRegressor(random_state = 101) \n",
    "\n",
    "rfr = ensemble.RandomForestRegressor(random_state = 101)\n",
    "\n",
    "etr = ensemble.ExtraTreesRegressor(random_state = 101)\n",
    "\n",
    "abr = ensemble.AdaBoostRegressor(random_state = 101)\n",
    "\n",
    "br = ensemble.BaggingRegressor(random_state = 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Grid search XGBoost\n",
    "\n",
    "param_grid = { \"n_estimators\"      : [1, 25],\n",
    "           \"learning_rate\"      : [0.01, 0.2],\n",
    "           \"max_depth\"         : [5, 20],\n",
    "           \"subsample\" : [0.1, 0.99] ,\n",
    "           \"colsample_bytree\": [0.01, 0.9]}\n",
    "\n",
    "# n_jobs=1, gamma=0, min_child_weight=1, max_delta_step=0, colsample_bylevel=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5\n",
    "\n",
    "# INSERT YOUR MODEL HERE WITH NO PARAMETERS DEFINED EXCEPT RANDOM SEED\n",
    "grid_search = GridSearchCV(xgbr, param_grid, n_jobs=-1, cv=2)\n",
    "grid_search.fit(features_train, labels_train) # right datasets???\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Grid search GradientBoost\n",
    " \n",
    "param_grid = { \"n_estimators\"      : [275, 325],\n",
    "           \"learning_rate\"      : [0.0001, 0.1],\n",
    "           \"max_depth\"         : [5, 15],\n",
    "           \"min_samples_leaf\": [3,6],\n",
    "           \"subsample\" : [0.1, 0.99] ,\n",
    "           \"warm_start\": [True, False]}\n",
    "\n",
    "# max_features=None, alpha=0.9, verbose=0, \"min_samples_split\": [1,4],\n",
    "\n",
    "# INSERT YOUR MODEL HERE WITH NO PARAMETERS DEFINED EXCEPT RANDOM SEED\n",
    "grid_search = GridSearchCV(gbr, param_grid, n_jobs=-1, cv=2)\n",
    "grid_search.fit(features_train, labels_train)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Grid search RandomForests\n",
    " \n",
    "param_grid = { \"n_estimators\"      : [275, 325],\n",
    "           \"max_depth\"         : [5, 15],\n",
    "           \"min_samples_leaf\": [3,6],\n",
    "           \"warm_start\": [True, False]}\n",
    "\n",
    "# max_features=None, alpha=0.9, verbose=0, \"min_samples_split\": [1,4],\n",
    "\n",
    "# INSERT YOUR MODEL HERE WITH NO PARAMETERS DEFINED EXCEPT RANDOM SEED\n",
    "grid_search = GridSearchCV(rfr, param_grid, n_jobs=-1, cv=2)\n",
    "grid_search.fit(features_train, labels_train)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Grid search ExtraTrees\n",
    " \n",
    "param_grid = { \"n_estimators\"      : [225, 275],\n",
    "           \"max_depth\"         : [5, 15],\n",
    "           \"min_samples_leaf\": [3, 6],\n",
    "           \"bootstrap\": [True, False],\n",
    "           \"warm_start\": [True, False]}           \n",
    "\n",
    "# INSERT YOUR MODEL HERE WITH NO PARAMETERS DEFINED EXCEPT RANDOM SEED\n",
    "grid_search = GridSearchCV(etr, param_grid, n_jobs=-1, cv=2)\n",
    "grid_search.fit(features_train, labels_train)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Grid search AdaBoostingTrees\n",
    " \n",
    "param_grid = { \"n_estimators\"      : [200, 500],\n",
    "           \"learning_rate\"      : [0.00001, 0.1]}           \n",
    "\n",
    "# INSERT YOUR MODEL HERE WITH NO PARAMETERS DEFINED EXCEPT RANDOM SEED\n",
    "grid_search = GridSearchCV(abr, param_grid, n_jobs=-1, cv=2)\n",
    "grid_search.fit(features_train, labels_train)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Grid search BoostingTrees\n",
    " \n",
    "param_grid = { \"n_estimators\"      : [275, 325],\n",
    "           \"max_samples\"         : [5, 15],\n",
    "           \"n_jobs\" : [1, 5] ,\n",
    "           \"bootstrap\": [True, False],\n",
    "           \"warm_start\": [True, False]}           \n",
    "\n",
    "# INSERT YOUR MODEL HERE WITH NO PARAMETERS DEFINED EXCEPT RANDOM SEED\n",
    "grid_search = GridSearchCV(br, param_grid, n_jobs=-1, cv=2)\n",
    "grid_search.fit(features_train, labels_train)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#estimator.get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lm = linear_model.LinearRegression()\n",
    "\n",
    "dnn = tf.contrib.learn.DNNRegressor(feature_columns=feature_columns, hidden_units=[20,15], \n",
    "                                    activation_fn=tf.nn.relu6) #, tf_random_seed = 101)\n",
    "\n",
    "xgbr = xgb.XGBRegressor(seed = 101, n_estimators = 5, subsample = 0.9, learning_rate = 0.1, colsample_bytree = 0.1, max_depth = 15) \n",
    "#{'n_estimators': 5, 'subsample': 0.9, 'learning_rate': 0.1, 'colsample_bytree': 0.1, 'max_depth': 15}\n",
    "\n",
    "gbr = ensemble.GradientBoostingRegressor(random_state = 101, n_estimators = 300, warm_start = True, max_depth = 10, learning_rate = 0.001, min_samples_leaf = 4, subsample = 0.9) \n",
    "#{'n_estimators': 300, 'warm_start': True, 'max_depth': 10, 'learning_rate': 0.001, 'min_samples_leaf': 4, 'subsample': 0.9}\n",
    "\n",
    "rfr = ensemble.RandomForestRegressor(random_state = 101, n_estimators = 300, warm_start = True, min_samples_leaf = 4, max_depth = 10)\n",
    "#{'n_estimators': 300, 'warm_start': True, 'min_samples_leaf': 4, 'max_depth': 10}\n",
    "\n",
    "etr = ensemble.ExtraTreesRegressor(random_state = 101, n_estimators = 250, bootstrap = False, warm_start = True, min_samples_leaf = 4, max_depth = 10)\n",
    "#{'n_estimators': 250, 'bootstrap': False, 'warm_start': True, 'min_samples_leaf': 4, 'max_depth': 10}\n",
    "\n",
    "abr = ensemble.AdaBoostRegressor(random_state = 101, n_estimators = 500, learning_rate = 0.00001)\n",
    "#{'n_estimators': 250, 'learning_rate': 0.001}\n",
    "\n",
    "br = ensemble.BaggingRegressor(random_state = 101, n_estimators = 300, n_jobs = 1, warm_start = True, max_samples = 10, bootstrap = False)\n",
    "#{'n_estimators': 300, 'n_jobs': 1, 'warm_start': True, 'max_samples': 10, 'bootstrap': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lm.fit(x_train, y_train)\n",
    "dnn.fit(x_train, y_train, steps=250)\n",
    "gbr.fit(x_train, y_train)\n",
    "rfr.fit(x_train, y_train)\n",
    "etr.fit(x_train, y_train)\n",
    "abr.fit(x_train, y_train)\n",
    "br.fit(x_train, y_train)\n",
    "xgbr.fit(x_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('LM Training Error:', metrics.mean_absolute_error(y_train, lm.predict(x_train)))\n",
    "print('LM Validation Error:', metrics.mean_absolute_error(y_val, lm.predict(x_val)))\n",
    "diff1 = metrics.mean_absolute_error(y_train, lm.predict(x_train))-metrics.mean_absolute_error(y_val, lm.predict(x_val))\n",
    "print('Diff:', diff1,'\\n')\n",
    "\n",
    "print('XGB Training Error:', metrics.mean_absolute_error(y_train, xgbr.predict(x_train)))\n",
    "print('XGB Validation Error:', metrics.mean_absolute_error(y_val, xgbr.predict(x_val)))\n",
    "diff2 = metrics.mean_absolute_error(y_train, xgbr.predict(x_train))-metrics.mean_absolute_error(y_val, xgbr.predict(x_val))\n",
    "print('Diff:', diff2,'\\n')\n",
    "\n",
    "print('GBR Training Error:', metrics.mean_absolute_error(y_train, gbr.predict(x_train)))\n",
    "print('GBR  Validation Error:', metrics.mean_absolute_error(y_val, gbr.predict(x_val)))\n",
    "diff3 = metrics.mean_absolute_error(y_train, gbr.predict(x_train))-metrics.mean_absolute_error(y_val, gbr.predict(x_val))\n",
    "print('Diff:', diff3,'\\n')\n",
    "\n",
    "print('RFR Training Error:', metrics.mean_absolute_error(y_train, rfr.predict(x_train)))\n",
    "print('RFR Validation Error:', metrics.mean_absolute_error(y_val, rfr.predict(x_val)))\n",
    "diff4 = metrics.mean_absolute_error(y_train, rfr.predict(x_train))-metrics.mean_absolute_error(y_val, rfr.predict(x_val))\n",
    "print('Diff:', diff4,'\\n')\n",
    "\n",
    "print('ETR Training Error:', metrics.mean_absolute_error(y_train, etr.predict(x_train)))\n",
    "print('ETR Validation Error:', metrics.mean_absolute_error(y_val, etr.predict(x_val)))\n",
    "diff5 = metrics.mean_absolute_error(y_train, etr.predict(x_train))-metrics.mean_absolute_error(y_val, etr.predict(x_val))\n",
    "print('Diff:', diff5,'\\n')\n",
    "\n",
    "print('ABR Training Error:', metrics.mean_absolute_error(y_train, abr.predict(x_train)))\n",
    "print('ABR Validation Error:', metrics.mean_absolute_error(y_val, abr.predict(x_val)))\n",
    "diff6 = metrics.mean_absolute_error(y_train, abr.predict(x_train))-metrics.mean_absolute_error(y_val, abr.predict(x_val))\n",
    "print('Diff:', diff6,'\\n')\n",
    "\n",
    "print('BR Training Error:', metrics.mean_absolute_error(y_train, br.predict(x_train)))\n",
    "print('BR Validation Error:', metrics.mean_absolute_error(y_val, br.predict(x_val)))\n",
    "diff7 = metrics.mean_absolute_error(y_train, br.predict(x_train))-metrics.mean_absolute_error(y_val, br.predict(x_val))\n",
    "print('Diff:', diff7,'\\n')\n",
    "\n",
    "print('DNN Training Error:', metrics.mean_absolute_error(y_train, list(dnn.predict(x_train, as_iterable=True))))\n",
    "#print('DNN Validation Error:', metrics.mean_absolute_error(y_val, list(dnn.predict(x_val, as_iterable=True)),'\\n'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lm.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dnn.fit(features_train, labels_train, steps = 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgbr.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gbr.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rfr.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "etr.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "abr.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "br.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_lm = lm.predict(features_train)\n",
    "final_dnn = dnn.predict(features_train, as_iterable=True)\n",
    "final_xgbr = xgbr.predict(features_train)\n",
    "final_gbr = gbr.predict(features_train)\n",
    "final_rfr = rfr.predict(features_train)\n",
    "final_etr = etr.predict(features_train)\n",
    "final_abr = abr.predict(features_train)\n",
    "final_br = br.predict(features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_dnn = np.asarray(final_dnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_dnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_lm[final_lm < 0] = 0\n",
    "final_lm = final_lm.astype(dtype='int64')\n",
    "\n",
    "#final_dnn[final_dnn < 0] = 0\n",
    "#final_dnn = final_dnn.astype(dtype='int64')\n",
    "\n",
    "final_xgbr[final_xgbr < 0] = 0\n",
    "final_xgbr = final_xgbr.astype(dtype='int64')\n",
    "\n",
    "final_gbr[final_gbr < 0] = 0\n",
    "final_gbr = final_gbr.astype(dtype='int64')\n",
    "\n",
    "final_rfr[final_rfr < 0] = 0\n",
    "final_rfr = final_rfr.astype(dtype='int64')\n",
    "\n",
    "final_etr[final_etr < 0] = 0\n",
    "final_etr = final_etr.astype(dtype='int64')\n",
    "\n",
    "final_abr[final_abr < 0] = 0\n",
    "final_abr = final_abr.astype(dtype='int64')\n",
    "\n",
    "final_br[final_br < 0] = 0\n",
    "final_br = final_br.astype(dtype='int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FINAL = np.hstack((final_lm, np.reshape(final_xgbr, (1456,1)), np.reshape(final_gbr, (1456,1)), \n",
    "                   np.reshape(final_rfr, (1456,1)), np.reshape(final_etr, (1456,1)), \n",
    "                   np.reshape(final_abr, (1456,1)), np.reshape(final_br, (1456,1)))) \n",
    "\n",
    "'''features_train)) #, final_dnn'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgbr2 = xgb.XGBRegressor(seed = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Grid search XGBoost\n",
    "\n",
    "param_grid = { \"n_estimators\"      : [1, 25],\n",
    "           \"learning_rate\"      : [0.01, 0.2],\n",
    "           \"max_depth\"         : [5, 20],\n",
    "           \"subsample\" : [0.1, 0.99] ,\n",
    "           \"colsample_bytree\": [0.01, 0.9]}\n",
    "\n",
    "# n_jobs=1, gamma=0, min_child_weight=1, max_delta_step=0, colsample_bylevel=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5\n",
    "\n",
    "# INSERT YOUR MODEL HERE WITH NO PARAMETERS DEFINED EXCEPT RANDOM SEED\n",
    "grid_search = GridSearchCV(xgbr2, param_grid, n_jobs=-1, cv=2)\n",
    "grid_search.fit(features_train, labels_train) # right datasets???\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgbr2 = xgb.XGBRegressor(seed = 100, n_estimators = 5, subsample = 0.9, learning_rate = 0.1, colsample_bytree = 0.5, max_depth = 15) \n",
    "#{'n_estimators': 5, 'subsample': 0.9, 'learning_rate': 0.1, 'colsample_bytree': 0.1, 'max_depth': 15}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xgbr2.fit(FINAL, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_xgbr2 = xgbr2.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_xgbr2[final_xgbr2 < 0] = 0\n",
    "final_xgbr2 = final_xgbr2.astype(dtype='int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"final_ensemble.csv\", FINAL, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Stacker(xgbr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = ensemble.GradientBoostingRegressor()\n",
    "l = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x[: 80, 0] = Stacker(g).fit_transform(x0[: 80, :], y[: 80])[:, 0]\n",
    "x[: 80, 1] = Stacker(l).fit_transform(x1[: 80, :], y[: 80])[:, 0]\n",
    "\n",
    "u = linear_model.LinearRegression().fit(x[: 80, :], y[: 80])\n",
    "\n",
    "x[80: , 0] = Stacker(g).fit(x0[: 80, :], y[: 80]).transform(x0[80:, :])\n",
    "x[80: , 1] = Stacker(l).fit(x1[: 80, :], y[: 80]).transform(x1[80:, :])\n",
    "\n",
    "metrics.r2_score(\n",
    "        y[80: ],\n",
    "        u.predict(x[80:, :]))\n",
    "    0.992196564279"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
