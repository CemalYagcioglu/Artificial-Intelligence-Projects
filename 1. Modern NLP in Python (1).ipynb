{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./media/Logo Element-01.jpg\"><h1 align=\"center\">Applied Artificial Intelligence</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Before we get started...\n",
    "__whois?__\n",
    "- Raymond Ellis\n",
    "- Data Scientist @ Vodacom\n",
    "- twitter: @igelundfuchs\n",
    "- Wikimedia: Raymond Ellis\n",
    "- keybase.io/raymondellis\n",
    "- raymond.ellis@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./media/Deep_Learning_Icons_R5_PNG.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./media/4-Vs-of-big-data.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./media/gpu-computing-in-higher-education-and-research-4-638.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./media/doctor.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./media/robolution.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./media/ff.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./media/guardian.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./media/tractor.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./media/otto.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./media/kiva.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./media/amazongo.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./media/spacex.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./media/time.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./media/nips.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./media/CDF.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./media/milestones.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./media/airpods.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./media/neuron.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./media/zoo.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./media/faces.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./media/convnets.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./media/dreams.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./media/birds.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./media/zoo.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h1 align=\"center\">Modern NLP in Python</h1>\n",
    "<h2 align=\"center\">- or -</h2>\n",
    "<h1 align=\"center\">What you can learn about food by analyzing 4 million Yelp reviews</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "_Note: A version of this notebook was presented as a tutorial during the [PyData DC 2016 conference] by Partrick Harrison (http://pydata.org/dc2016/schedule/presentation/11/). To view the video of that presentation on YouTube, see [here](https://www.youtube.com/watch?v=6zm9NC9uRkk)._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Our Trail Map\n",
    "This tutorial features an end-to-end data science & natural language processing pipeline, starting with **raw data** and running through **preparing**, **modeling**, **visualizing**, and **analyzing** the data. We'll touch on the following points:\n",
    "1. A tour of the dataset\n",
    "1. Introduction to text processing with spaCy\n",
    "1. Automatic phrase modeling\n",
    "1. Topic modeling with LDA\n",
    "1. Visualizing topic models with pyLDAvis\n",
    "1. Word vector models with word2vec\n",
    "1. Visualizing word2vec with t-SNE\n",
    "\n",
    "...and we might even learn a thing or two about Python along the way.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Yelp Dataset\n",
    "[**The Yelp Dataset**](https://www.yelp.com/dataset_challenge/) is a dataset published by the business review service [Yelp](http://yelp.com) for academic research and educational purposes. I really like the Yelp dataset as a subject for machine learning and natural language processing demos, because it's big (but not so big that you need your own data center to process it), well-connected, and anyone can relate to it &mdash; it's largely about food, after all!\n",
    "\n",
    "**Note:** If you'd like to execute this notebook interactively on your local machine, you'll need to download your own copy of the Yelp dataset. If you're reviewing a static copy of the notebook online, you can skip this step. Here's how to get the dataset:\n",
    "1. Please visit the Yelp dataset webpage [here](https://www.yelp.com/dataset_challenge/)\n",
    "1. Click \"Get the Data\"\n",
    "1. Please review, agree to, and respect Yelp's terms of use!\n",
    "1. The dataset downloads as a compressed .tgz file; uncompress it\n",
    "1. Place the uncompressed dataset files (*yelp_academic_dataset_business.json*, etc.) in a directory named *yelp_dataset_challenge_academic_dataset*\n",
    "1. Place the *yelp_dataset_challenge_academic_dataset* within the *data* directory in the *Modern NLP in Python* project folder\n",
    "\n",
    "That's it! You're ready to go.\n",
    "\n",
    "The current iteration of the Yelp dataset (as of this demo) consists of the following data:\n",
    "- __1M__ users\n",
    "- __144K__ businesses\n",
    "- __4.1M__ user reviews\n",
    "\n",
    "The data is provided in a handful of files in _.json_ format. We'll be using the following files for our demo:\n",
    "- __yelp\\_academic\\_dataset\\_business.json__ &mdash; _the records for individual businesses_\n",
    "- __yelp\\_academic\\_dataset\\_review.json__ &mdash; _the records for reviews users wrote about businesses_\n",
    "\n",
    "The files are text files (UTF-8) with one _json object_ per line, each one corresponding to an individual data record. Let's take a look at a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"business_id\":\"0DI8Dt2PJp07XkVvIElIcQ\",\"name\":\"Innovative Vapors\",\"neighborhood\":\"\",\"address\":\"227 E Baseline Rd, Ste J2\",\"city\":\"Tempe\",\"state\":\"AZ\",\"postal_code\":\"85283\",\"latitude\":33.3782141,\"longitude\":-111.936102,\"stars\":4.5,\"review_count\":17,\"is_open\":0,\"attributes\":[\"BikeParking: True\",\"BusinessAcceptsBitcoin: False\",\"BusinessAcceptsCreditCards: True\",\"BusinessParking: {'garage': False, 'street': False, 'validated': False, 'lot': True, 'valet': False}\",\"DogsAllowed: False\",\"RestaurantsPriceRange2: 2\",\"WheelchairAccessible: True\"],\"categories\":[\"Tobacco Shops\",\"Nightlife\",\"Vape Shops\",\"Shopping\"],\"hours\":[\"Monday 11:0-21:0\",\"Tuesday 11:0-21:0\",\"Wednesday 11:0-21:0\",\"Thursday 11:0-21:0\",\"Friday 11:0-22:0\",\"Saturday 10:0-22:0\",\"Sunday 11:0-18:0\"],\"type\":\"business\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import codecs\n",
    "\n",
    "data_directory = os.path.join('.', 'input', 'yelp_dataset_challenge_round9')\n",
    "\n",
    "businesses_filepath = os.path.join(data_directory, \n",
    "                                   'yelp_academic_dataset_business.json')\n",
    "\n",
    "with codecs.open(businesses_filepath, encoding='utf_8') as f:\n",
    "    first_business_record = f.readline() \n",
    "\n",
    "print(first_business_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The business records consist of _key, value_ pairs containing information about the particular business. A few attributes we'll be interested in for this demo include:\n",
    "- __business\\_id__ &mdash; _unique identifier for businesses_\n",
    "- __categories__ &mdash; _an array containing relevant category values of businesses_\n",
    "\n",
    "The _categories_ attribute is of special interest. This demo will focus on restaurants, which are indicated by the presence of the _Restaurant_ tag in the _categories_ array. In addition, the _categories_ array may contain more detailed information about restaurants, such as the type of food they serve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The review records are stored in a similar manner &mdash; _key, value_ pairs containing information about the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"review_id\":\"NxL8SIC5yqOdnlXCg18IBg\",\"user_id\":\"KpkOkG6RIf4Ra25Lhhxf1A\",\"business_id\":\"2aFiy99vNLklCx3T_tGS9A\",\"stars\":5,\"date\":\"2011-10-10\",\"text\":\"If you enjoy service by someone who is as competent as he is personable, I would recommend Corey Kaplan highly. The time he has spent here has been very productive and working with him educational and enjoyable. I hope not to need him again (though this is highly unlikely) but knowing he is there if I do is very nice. By the way, I'm not from El Centro, CA. but Scottsdale, AZ.\",\"useful\":0,\"funny\":0,\"cool\":0,\"type\":\"review\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review_json_filepath = os.path.join(data_directory,\n",
    "                                    'yelp_academic_dataset_review.json')\n",
    "\n",
    "with codecs.open(review_json_filepath, encoding='utf_8') as f:\n",
    "    first_review_record = f.readline()\n",
    "    \n",
    "print(first_review_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A few attributes of note on the review records:\n",
    "- __business\\_id__ &mdash; _indicates which business the review is about_\n",
    "- __text__ &mdash; _the natural language text the user wrote_\n",
    "\n",
    "The _text_ attribute will be our focus today!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "_json_ is a handy file format for data interchange, but it's typically not the most usable for any sort of modeling work. Let's do a bit more data preparation to get our data in a more usable format. Our next code block will do the following:\n",
    "1. Read in each business record and convert it to a Python `dict`\n",
    "2. Filter out business records that aren't about restaurants (i.e., not in the \"Restaurant\" category)\n",
    "3. Create a `frozenset` of the business IDs for restaurants, which we'll use in the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "restaurant_ids = set()\n",
    "\n",
    "# open the businesses file\n",
    "with codecs.open(businesses_filepath, encoding='utf_8') as f:\n",
    "    \n",
    "    # iterate through each line (json record) in the file\n",
    "    for business_json in f:\n",
    "        \n",
    "        # convert the json record to a Python dict\n",
    "        business = json.loads(business_json)\n",
    "        \n",
    "        # if this business is not a restaurant, skip to the next one\n",
    "        #if u'Restaurants' not in business[u'categories']:\n",
    "        #    continue\n",
    "            \n",
    "        # add the restaurant business id to our restaurant_ids set\n",
    "        #restaurant_ids.add(business[u'business_id'])\n",
    "        if business[u'categories'] is not None:\n",
    "            if u'Restaurants' in business[u'categories']:\n",
    "                restaurant_ids.add(business[u'business_id'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48,485 restaurants in the dataset.\n"
     ]
    }
   ],
   "source": [
    "# turn restaurant_ids into a frozenset, as we don't need to change it anymore\n",
    "restaurant_ids = frozenset(restaurant_ids)\n",
    "\n",
    "# print the number of unique restaurant ids in the dataset\n",
    "print('{:,}'.format(len(restaurant_ids)), u'restaurants in the dataset.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3>Next, we will create a new file that contains only the text from reviews about restaurants, with one review per line in the file.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "intermediate_directory = os.path.join('.', 'intermediate')\n",
    "\n",
    "review_txt_filepath = os.path.join(intermediate_directory,\n",
    "                                   'review_text_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text from 2,579,047 restaurant reviews in the txt file.\n",
      "CPU times: user 1min 39s, sys: 3.4 s, total: 1min 42s\n",
      "Wall time: 3min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import io\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if 0 == 1:\n",
    "    \n",
    "    review_count = 0\n",
    "\n",
    "    # create & open a new file in write mode\n",
    "    with io.open(review_txt_filepath, 'w', encoding='utf_8') as review_txt_file:\n",
    "\n",
    "        # open the existing review json file\n",
    "        with codecs.open(review_json_filepath, encoding='utf_8') as review_json_file:\n",
    "\n",
    "            # loop through all reviews in the existing file and convert to dict\n",
    "            for review_json in review_json_file:\n",
    "                review = json.loads(review_json)\n",
    "\n",
    "                # if this review is not about a restaurant, skip to the next one\n",
    "                if review[u'business_id'] not in restaurant_ids:\n",
    "                    continue\n",
    "\n",
    "                # write the restaurant review as a line in the new file\n",
    "                # escape newline characters in the original review text\n",
    "                review_txt_file.write(review[u'text'].replace('\\n', '\\\\n') + '\\n')\n",
    "                review_count += 1\n",
    "\n",
    "    print(u'''Text from {:,} restaurant reviews\n",
    "              written to the new txt file.'''.format(review_count))\n",
    "    \n",
    "else:\n",
    "    \n",
    "    with codecs.open(review_txt_filepath, encoding='utf_8') as review_txt_file:\n",
    "        for review_count, line in enumerate(review_txt_file):\n",
    "            pass\n",
    "        \n",
    "    print(u'Text from {:,} restaurant reviews in the txt file.'.format(review_count + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## spaCy &mdash; Industrial-Strength NLP in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![spaCy](https://s3.amazonaws.com/skipgram-images/spaCy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[**spaCy**](https://spacy.io) is an industrial-strength natural language processing (_NLP_) library for Python. spaCy's goal is to take recent advancements in natural language processing out of research papers and put them in the hands of users to build production software.\n",
    "\n",
    "spaCy handles many tasks commonly associated with building an end-to-end natural language processing pipeline:\n",
    "- Tokenization\n",
    "- Text normalization, such as lowercasing, stemming/lemmatization\n",
    "- Part-of-speech tagging\n",
    "- Syntactic dependency parsing\n",
    "- Sentence boundary detection\n",
    "- Named entity recognition and annotation\n",
    "\n",
    "In the \"batteries included\" Python tradition, spaCy contains built-in data and models which you can use out-of-the-box for processing general-purpose English language text:\n",
    "- Large English vocabulary, including stopword lists\n",
    "- Token \"probabilities\"\n",
    "- Word vectors\n",
    "\n",
    "spaCy is written in optimized Cython, which means it's _fast_. According to a few independent sources, it's the fastest syntactic parser available in any language. Key pieces of the spaCy parsing pipeline are written in pure C, enabling efficient multithreading (i.e., spaCy can release the _GIL_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Was boozin and cruisin with my canine comrade. Went all the way out to Hudson via Brecksville/82. Looking for anything to Yelp. Nothing. Not even in the strip mall I used to hit in Hudson on my dry cleaning route back in the day. There was a deli, but it opened at fucking 11! Who even wants to go to a deli after 11? Weak. Breezed through Peninsula. Winking Lizard? Bah. Ended up backtracking through Brecksville and I remembered an old friend. London Pickle Works! Was it still there some four years after I stopped in while house shopping? Yup. Incidentally, investing in a condo down there probably would have made more sense than buying a house in Lakewood. Live and learn.\n",
      "\n",
      "Cozied up to the bar and ordered a Killians, plus cheesesteak to go(for pooch). Like the Unicorn, food was amazingly done before I could chug my brew. I could get used to this kind of service. The bar was all old men. Matt, I don't know how you do it. They were watching Fox news for chrissakes!! Total was eight dollars! Holy shit. I dropped a three dollar tip on that barmaid's ass. How could I not? A real meal at fast food prices, at fast food speed, served by someone not under house arrest with work privileges. Busted open the styro box. Big sandwich with mushrooms, onions, cheez and beef sliced so thin I swear it was a Steak'um. I never ate those as a kid, but I remember one time my friend had one in his bedroom where we were playing NES. He didn't want it, so we threw it all around the room. Fries were awesome. Also got a pickle. That's eight dollars with the beer, just to recap. Great value, nice location, old men. I won't wait four more years to come back.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with codecs.open(review_txt_filepath, encoding='utf_8') as f:\n",
    "    sample_review = list(it.islice(f, 8, 9))[0]\n",
    "    sample_review = sample_review.replace('\\\\n', '\\n')\n",
    "        \n",
    "print(sample_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3>Hand the review text to spaCy, and be prepared to wait...</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 63.9 ms, sys: 49.6 ms, total: 114 ms\n",
      "Wall time: 315 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "parsed_review = nlp(sample_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's take a look at what we got during that time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Was boozin and cruisin with my canine comrade. Went all the way out to Hudson via Brecksville/82. Looking for anything to Yelp. Nothing. Not even in the strip mall I used to hit in Hudson on my dry cleaning route back in the day. There was a deli, but it opened at fucking 11! Who even wants to go to a deli after 11? Weak. Breezed through Peninsula. Winking Lizard? Bah. Ended up backtracking through Brecksville and I remembered an old friend. London Pickle Works! Was it still there some four years after I stopped in while house shopping? Yup. Incidentally, investing in a condo down there probably would have made more sense than buying a house in Lakewood. Live and learn.\n",
      "\n",
      "Cozied up to the bar and ordered a Killians, plus cheesesteak to go(for pooch). Like the Unicorn, food was amazingly done before I could chug my brew. I could get used to this kind of service. The bar was all old men. Matt, I don't know how you do it. They were watching Fox news for chrissakes!! Total was eight dollars! Holy shit. I dropped a three dollar tip on that barmaid's ass. How could I not? A real meal at fast food prices, at fast food speed, served by someone not under house arrest with work privileges. Busted open the styro box. Big sandwich with mushrooms, onions, cheez and beef sliced so thin I swear it was a Steak'um. I never ate those as a kid, but I remember one time my friend had one in his bedroom where we were playing NES. He didn't want it, so we threw it all around the room. Fries were awesome. Also got a pickle. That's eight dollars with the beer, just to recap. Great value, nice location, old men. I won't wait four more years to come back.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(parsed_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Looks the same! What happened under the hood?\n",
    "\n",
    "What about sentence detection and segmentation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1:\n",
      "Was boozin and cruisin with my canine comrade.\n",
      "\n",
      "Sentence 2:\n",
      "Went all the way out to Hudson via Brecksville/82.\n",
      "\n",
      "Sentence 3:\n",
      "Looking for anything to Yelp.\n",
      "\n",
      "Sentence 4:\n",
      "Nothing.\n",
      "\n",
      "Sentence 5:\n",
      "Not even in the strip mall I used to hit in Hudson on my dry cleaning route back in the day.\n",
      "\n",
      "Sentence 6:\n",
      "There was a deli, but it opened at fucking 11!\n",
      "\n",
      "Sentence 7:\n",
      "Who even wants to go to a deli after 11?\n",
      "\n",
      "Sentence 8:\n",
      "Weak\n",
      "\n",
      "Sentence 9:\n",
      ". Breezed through Peninsula.\n",
      "\n",
      "Sentence 10:\n",
      "Winking Lizard?\n",
      "\n",
      "Sentence 11:\n",
      "Bah.\n",
      "\n",
      "Sentence 12:\n",
      "Ended up backtracking through Brecksville and I remembered an old friend.\n",
      "\n",
      "Sentence 13:\n",
      "London Pickle Works!\n",
      "\n",
      "Sentence 14:\n",
      "Was it still there some four years after I stopped in while house shopping?\n",
      "\n",
      "Sentence 15:\n",
      "Yup.\n",
      "\n",
      "Sentence 16:\n",
      "Incidentally, investing in a condo down there probably would have made more sense than buying a house in Lakewood.\n",
      "\n",
      "Sentence 17:\n",
      "Live and learn.\n",
      "\n",
      "\n",
      "\n",
      "Sentence 18:\n",
      "Cozied up to the bar and ordered a Killians, plus cheesesteak to go(for pooch).\n",
      "\n",
      "Sentence 19:\n",
      "Like the Unicorn, food was amazingly done before I could chug my brew. I could get used to this kind of service.\n",
      "\n",
      "Sentence 20:\n",
      "The bar was all old men.\n",
      "\n",
      "Sentence 21:\n",
      "Matt, I don't know how you do it.\n",
      "\n",
      "Sentence 22:\n",
      "They were watching Fox news for chrissakes!!\n",
      "\n",
      "Sentence 23:\n",
      "Total was eight dollars!\n",
      "\n",
      "Sentence 24:\n",
      "Holy shit.\n",
      "\n",
      "Sentence 25:\n",
      "I dropped a three dollar tip on that barmaid's ass.\n",
      "\n",
      "Sentence 26:\n",
      "How could I not?\n",
      "\n",
      "Sentence 27:\n",
      "A real meal at fast food prices, at fast food speed, served by someone not under house arrest with work privileges. Busted open the styro box.\n",
      "\n",
      "Sentence 28:\n",
      "Big sandwich with mushrooms, onions, cheez and beef sliced so thin I swear it was a Steak'um.\n",
      "\n",
      "Sentence 29:\n",
      "I never ate those as a kid, but I remember one time my friend had one in his bedroom where we were playing NES.\n",
      "\n",
      "Sentence 30:\n",
      "He didn't want it, so we threw it all around the room.\n",
      "\n",
      "Sentence 31:\n",
      "Fries were awesome.\n",
      "\n",
      "Sentence 32:\n",
      "Also got a pickle.\n",
      "\n",
      "Sentence 33:\n",
      "That's eight dollars with the beer, just to recap.\n",
      "\n",
      "Sentence 34:\n",
      "Great value, nice location, old men.\n",
      "\n",
      "Sentence 35:\n",
      "I won't wait four more years to come back.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num, sentence in enumerate(parsed_review.sents):\n",
    "    print('Sentence {}:'.format(num + 1))\n",
    "    print(sentence)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Named entity detection</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity 1: Hudson - PERSON\n",
      "\n",
      "Entity 2: Yelp - PERSON\n",
      "\n",
      "Entity 3: Hudson - PERSON\n",
      "\n",
      "Entity 4: the day - DATE\n",
      "\n",
      "Entity 5: 11 - DATE\n",
      "\n",
      "Entity 6: 11 - CARDINAL\n",
      "\n",
      "Entity 7: Peninsula - PERSON\n",
      "\n",
      "Entity 8: Brecksville - GPE\n",
      "\n",
      "Entity 9: London Pickle Works - ORG\n",
      "\n",
      "Entity 10: some four years - DATE\n",
      "\n",
      "Entity 11: Lakewood - GPE\n",
      "\n",
      "Entity 12: Live - ORG\n",
      "\n",
      "Entity 13: Killians - NORP\n",
      "\n",
      "Entity 14: Unicorn - GPE\n",
      "\n",
      "Entity 15: Matt - PERSON\n",
      "\n",
      "Entity 16: Fox - ORG\n",
      "\n",
      "Entity 17: three - CARDINAL\n",
      "\n",
      "Entity 18: Steak'um. - PERCENT\n",
      "\n",
      "Entity 19: one - CARDINAL\n",
      "\n",
      "Entity 20: one - CARDINAL\n",
      "\n",
      "Entity 21: NES - ORG\n",
      "\n",
      "Entity 22: Fries - PERSON\n",
      "\n",
      "Entity 23: eight dollars - MONEY\n",
      "\n",
      "Entity 24: four more years - DATE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num, entity in enumerate(parsed_review.ents):\n",
    "    print('Entity {}:'.format(num + 1), entity, '-', entity.label_)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Part of Speech Tagging</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_text</th>\n",
       "      <th>part_of_speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Was</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>boozin</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and</td>\n",
       "      <td>CCONJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cruisin</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>with</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>my</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>canine</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>comrade</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Went</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>all</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>way</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>out</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>to</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Hudson</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>via</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Brecksville/82</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Looking</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>for</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>anything</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>to</td>\n",
       "      <td>PART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Yelp</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Nothing</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Not</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>even</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>in</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>dollars</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>with</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>beer</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>just</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>to</td>\n",
       "      <td>PART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>recap</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>Great</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>value</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>nice</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>location</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>old</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>men</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>I</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>wo</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>n't</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>wait</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>four</td>\n",
       "      <td>NUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>more</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>years</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>to</td>\n",
       "      <td>PART</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>come</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>back</td>\n",
       "      <td>ADV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>\\n</td>\n",
       "      <td>SPACE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>369 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         token_text part_of_speech\n",
       "0               Was           VERB\n",
       "1            boozin           NOUN\n",
       "2               and          CCONJ\n",
       "3           cruisin           NOUN\n",
       "4              with            ADP\n",
       "5                my            ADJ\n",
       "6            canine            ADJ\n",
       "7           comrade           NOUN\n",
       "8                 .          PUNCT\n",
       "9              Went           VERB\n",
       "10              all            ADJ\n",
       "11              the            DET\n",
       "12              way           NOUN\n",
       "13              out            ADP\n",
       "14               to            ADP\n",
       "15           Hudson          PROPN\n",
       "16              via            ADP\n",
       "17   Brecksville/82          PROPN\n",
       "18                .          PUNCT\n",
       "19          Looking           VERB\n",
       "20              for            ADP\n",
       "21         anything           NOUN\n",
       "22               to           PART\n",
       "23             Yelp          PROPN\n",
       "24                .          PUNCT\n",
       "25          Nothing           NOUN\n",
       "26                .          PUNCT\n",
       "27              Not            ADV\n",
       "28             even            ADV\n",
       "29               in            ADP\n",
       "..              ...            ...\n",
       "339         dollars           NOUN\n",
       "340            with            ADP\n",
       "341             the            DET\n",
       "342            beer           NOUN\n",
       "343               ,          PUNCT\n",
       "344            just            ADV\n",
       "345              to           PART\n",
       "346           recap           VERB\n",
       "347               .          PUNCT\n",
       "348           Great            ADJ\n",
       "349           value           NOUN\n",
       "350               ,          PUNCT\n",
       "351            nice            ADJ\n",
       "352        location           NOUN\n",
       "353               ,          PUNCT\n",
       "354             old            ADJ\n",
       "355             men           NOUN\n",
       "356               .          PUNCT\n",
       "357               I           PRON\n",
       "358              wo           VERB\n",
       "359             n't            ADV\n",
       "360            wait           VERB\n",
       "361            four            NUM\n",
       "362            more            ADJ\n",
       "363           years           NOUN\n",
       "364              to           PART\n",
       "365            come           VERB\n",
       "366            back            ADV\n",
       "367               .          PUNCT\n",
       "368              \\n          SPACE\n",
       "\n",
       "[369 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_text = [token.orth_ for token in parsed_review]\n",
    "token_pos = [token.pos_ for token in parsed_review]\n",
    "\n",
    "pd.DataFrame(data=list(zip(token_text, token_pos)),\n",
    "             columns=['token_text', 'part_of_speech'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Text normalization, like stemming/lemmatization and shape analysis?</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_text</th>\n",
       "      <th>token_lemma</th>\n",
       "      <th>token_shape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Was</td>\n",
       "      <td>be</td>\n",
       "      <td>Xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>boozin</td>\n",
       "      <td>boozin</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cruisin</td>\n",
       "      <td>cruisin</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>with</td>\n",
       "      <td>with</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>my</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>canine</td>\n",
       "      <td>canine</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>comrade</td>\n",
       "      <td>comrade</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Went</td>\n",
       "      <td>go</td>\n",
       "      <td>Xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>way</td>\n",
       "      <td>way</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>out</td>\n",
       "      <td>out</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Hudson</td>\n",
       "      <td>hudson</td>\n",
       "      <td>Xxxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>via</td>\n",
       "      <td>via</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Brecksville/82</td>\n",
       "      <td>brecksville/82</td>\n",
       "      <td>Xxxxx/dd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Looking</td>\n",
       "      <td>look</td>\n",
       "      <td>Xxxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>for</td>\n",
       "      <td>for</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>anything</td>\n",
       "      <td>anything</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Yelp</td>\n",
       "      <td>yelp</td>\n",
       "      <td>Xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Nothing</td>\n",
       "      <td>nothing</td>\n",
       "      <td>Xxxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Not</td>\n",
       "      <td>not</td>\n",
       "      <td>Xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>even</td>\n",
       "      <td>even</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>dollars</td>\n",
       "      <td>dollar</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>with</td>\n",
       "      <td>with</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>beer</td>\n",
       "      <td>beer</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>just</td>\n",
       "      <td>just</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>recap</td>\n",
       "      <td>recap</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>Great</td>\n",
       "      <td>great</td>\n",
       "      <td>Xxxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>value</td>\n",
       "      <td>value</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>nice</td>\n",
       "      <td>nice</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>location</td>\n",
       "      <td>location</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>old</td>\n",
       "      <td>old</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>men</td>\n",
       "      <td>man</td>\n",
       "      <td>xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>I</td>\n",
       "      <td>-PRON-</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>wo</td>\n",
       "      <td>wo</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>n't</td>\n",
       "      <td>not</td>\n",
       "      <td>x'x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>wait</td>\n",
       "      <td>wait</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>four</td>\n",
       "      <td>four</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>more</td>\n",
       "      <td>more</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>years</td>\n",
       "      <td>year</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "      <td>xx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>come</td>\n",
       "      <td>come</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>back</td>\n",
       "      <td>back</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>\\n</td>\n",
       "      <td>\\n</td>\n",
       "      <td>\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>369 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         token_text     token_lemma token_shape\n",
       "0               Was              be         Xxx\n",
       "1            boozin          boozin        xxxx\n",
       "2               and             and         xxx\n",
       "3           cruisin         cruisin        xxxx\n",
       "4              with            with        xxxx\n",
       "5                my          -PRON-          xx\n",
       "6            canine          canine        xxxx\n",
       "7           comrade         comrade        xxxx\n",
       "8                 .               .           .\n",
       "9              Went              go        Xxxx\n",
       "10              all             all         xxx\n",
       "11              the             the         xxx\n",
       "12              way             way         xxx\n",
       "13              out             out         xxx\n",
       "14               to              to          xx\n",
       "15           Hudson          hudson       Xxxxx\n",
       "16              via             via         xxx\n",
       "17   Brecksville/82  brecksville/82    Xxxxx/dd\n",
       "18                .               .           .\n",
       "19          Looking            look       Xxxxx\n",
       "20              for             for         xxx\n",
       "21         anything        anything        xxxx\n",
       "22               to              to          xx\n",
       "23             Yelp            yelp        Xxxx\n",
       "24                .               .           .\n",
       "25          Nothing         nothing       Xxxxx\n",
       "26                .               .           .\n",
       "27              Not             not         Xxx\n",
       "28             even            even        xxxx\n",
       "29               in              in          xx\n",
       "..              ...             ...         ...\n",
       "339         dollars          dollar        xxxx\n",
       "340            with            with        xxxx\n",
       "341             the             the         xxx\n",
       "342            beer            beer        xxxx\n",
       "343               ,               ,           ,\n",
       "344            just            just        xxxx\n",
       "345              to              to          xx\n",
       "346           recap           recap        xxxx\n",
       "347               .               .           .\n",
       "348           Great           great       Xxxxx\n",
       "349           value           value        xxxx\n",
       "350               ,               ,           ,\n",
       "351            nice            nice        xxxx\n",
       "352        location        location        xxxx\n",
       "353               ,               ,           ,\n",
       "354             old             old         xxx\n",
       "355             men             man         xxx\n",
       "356               .               .           .\n",
       "357               I          -PRON-           X\n",
       "358              wo              wo          xx\n",
       "359             n't             not         x'x\n",
       "360            wait            wait        xxxx\n",
       "361            four            four        xxxx\n",
       "362            more            more        xxxx\n",
       "363           years            year        xxxx\n",
       "364              to              to          xx\n",
       "365            come            come        xxxx\n",
       "366            back            back        xxxx\n",
       "367               .               .           .\n",
       "368              \\n              \\n          \\n\n",
       "\n",
       "[369 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_lemma = [token.lemma_ for token in parsed_review]\n",
    "token_shape = [token.shape_ for token in parsed_review]\n",
    "\n",
    "pd.DataFrame(list(zip(token_text, token_lemma, token_shape)),\n",
    "             columns=['token_text', 'token_lemma', 'token_shape'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Token-level entity analysis</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_text</th>\n",
       "      <th>entity_type</th>\n",
       "      <th>inside_outside_begin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Was</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>boozin</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cruisin</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>with</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>my</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>canine</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>comrade</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Went</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>all</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>the</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>way</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>out</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>to</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Hudson</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>via</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Brecksville/82</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Looking</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>for</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>anything</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>to</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Yelp</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Nothing</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Not</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>even</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>in</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>dollars</td>\n",
       "      <td>MONEY</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>with</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>the</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>beer</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>,</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>just</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>to</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>recap</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>Great</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>value</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>,</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>nice</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>location</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>,</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>old</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>men</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>I</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>wo</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>n't</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>wait</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>four</td>\n",
       "      <td>DATE</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>more</td>\n",
       "      <td>DATE</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>years</td>\n",
       "      <td>DATE</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>to</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>come</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>back</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>\\n</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>369 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         token_text entity_type inside_outside_begin\n",
       "0               Was                                O\n",
       "1            boozin                                O\n",
       "2               and                                O\n",
       "3           cruisin                                O\n",
       "4              with                                O\n",
       "5                my                                O\n",
       "6            canine                                O\n",
       "7           comrade                                O\n",
       "8                 .                                O\n",
       "9              Went                                O\n",
       "10              all                                O\n",
       "11              the                                O\n",
       "12              way                                O\n",
       "13              out                                O\n",
       "14               to                                O\n",
       "15           Hudson      PERSON                    B\n",
       "16              via                                O\n",
       "17   Brecksville/82                                O\n",
       "18                .                                O\n",
       "19          Looking                                O\n",
       "20              for                                O\n",
       "21         anything                                O\n",
       "22               to                                O\n",
       "23             Yelp      PERSON                    B\n",
       "24                .                                O\n",
       "25          Nothing                                O\n",
       "26                .                                O\n",
       "27              Not                                O\n",
       "28             even                                O\n",
       "29               in                                O\n",
       "..              ...         ...                  ...\n",
       "339         dollars       MONEY                    I\n",
       "340            with                                O\n",
       "341             the                                O\n",
       "342            beer                                O\n",
       "343               ,                                O\n",
       "344            just                                O\n",
       "345              to                                O\n",
       "346           recap                                O\n",
       "347               .                                O\n",
       "348           Great                                O\n",
       "349           value                                O\n",
       "350               ,                                O\n",
       "351            nice                                O\n",
       "352        location                                O\n",
       "353               ,                                O\n",
       "354             old                                O\n",
       "355             men                                O\n",
       "356               .                                O\n",
       "357               I                                O\n",
       "358              wo                                O\n",
       "359             n't                                O\n",
       "360            wait                                O\n",
       "361            four        DATE                    B\n",
       "362            more        DATE                    I\n",
       "363           years        DATE                    I\n",
       "364              to                                O\n",
       "365            come                                O\n",
       "366            back                                O\n",
       "367               .                                O\n",
       "368              \\n                                O\n",
       "\n",
       "[369 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_entity_type = [token.ent_type_ for token in parsed_review]\n",
    "token_entity_iob = [token.ent_iob_ for token in parsed_review]\n",
    "\n",
    "pd.DataFrame(list(zip(token_text, token_entity_type, token_entity_iob)),\n",
    "             columns=['token_text', 'entity_type', 'inside_outside_begin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What about a variety of other token-level attributes, such as the relative frequency of tokens, and whether or not a token matches any of these categories?\n",
    "- stopword\n",
    "- punctuation\n",
    "- whitespace\n",
    "- represents a number\n",
    "- whether or not the token is included in spaCy's default vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>log_probability</th>\n",
       "      <th>stop?</th>\n",
       "      <th>punctuation?</th>\n",
       "      <th>whitespace?</th>\n",
       "      <th>number?</th>\n",
       "      <th>out of vocab.?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Was</td>\n",
       "      <td>-9.937194</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>boozin</td>\n",
       "      <td>-19.579313</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and</td>\n",
       "      <td>-4.195279</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cruisin</td>\n",
       "      <td>-19.579313</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>with</td>\n",
       "      <td>-5.363765</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>my</td>\n",
       "      <td>-5.918125</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>canine</td>\n",
       "      <td>-19.579313</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>comrade</td>\n",
       "      <td>-19.579313</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>.</td>\n",
       "      <td>-3.072948</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Went</td>\n",
       "      <td>-19.579313</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>all</td>\n",
       "      <td>-5.896713</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>the</td>\n",
       "      <td>-3.425446</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>way</td>\n",
       "      <td>-6.743997</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>out</td>\n",
       "      <td>-6.133558</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>to</td>\n",
       "      <td>-3.838520</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Hudson</td>\n",
       "      <td>-19.579313</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>via</td>\n",
       "      <td>-9.864821</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Brecksville/82</td>\n",
       "      <td>-19.579313</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>.</td>\n",
       "      <td>-3.072948</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Looking</td>\n",
       "      <td>-11.468876</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>for</td>\n",
       "      <td>-4.913970</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>anything</td>\n",
       "      <td>-7.498493</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>to</td>\n",
       "      <td>-3.838520</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Yelp</td>\n",
       "      <td>-19.579313</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>.</td>\n",
       "      <td>-3.072948</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Nothing</td>\n",
       "      <td>-9.858257</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>.</td>\n",
       "      <td>-3.072948</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Not</td>\n",
       "      <td>-7.758180</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>even</td>\n",
       "      <td>-6.705993</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>in</td>\n",
       "      <td>-4.585874</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>dollars</td>\n",
       "      <td>-8.924440</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>with</td>\n",
       "      <td>-5.363765</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>the</td>\n",
       "      <td>-3.425446</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>beer</td>\n",
       "      <td>-9.709007</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>,</td>\n",
       "      <td>-3.391480</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>just</td>\n",
       "      <td>-5.831590</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>to</td>\n",
       "      <td>-3.838520</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>recap</td>\n",
       "      <td>-19.579313</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>.</td>\n",
       "      <td>-3.072948</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>Great</td>\n",
       "      <td>-9.451549</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>value</td>\n",
       "      <td>-8.531943</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>,</td>\n",
       "      <td>-3.391480</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>nice</td>\n",
       "      <td>-8.462502</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>location</td>\n",
       "      <td>-10.667636</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>,</td>\n",
       "      <td>-3.391480</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>old</td>\n",
       "      <td>-7.795482</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>men</td>\n",
       "      <td>-8.724626</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>.</td>\n",
       "      <td>-3.072948</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>I</td>\n",
       "      <td>-4.064181</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>wo</td>\n",
       "      <td>-8.091730</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>n't</td>\n",
       "      <td>-4.883962</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>wait</td>\n",
       "      <td>-8.723573</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>four</td>\n",
       "      <td>-9.416868</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>more</td>\n",
       "      <td>-6.055923</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>years</td>\n",
       "      <td>-7.156296</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>to</td>\n",
       "      <td>-3.838520</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>come</td>\n",
       "      <td>-7.804588</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>back</td>\n",
       "      <td>-7.266421</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>.</td>\n",
       "      <td>-3.072948</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>\\n</td>\n",
       "      <td>-6.179879</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>369 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               text  log_probability stop? punctuation? whitespace? number?  \\\n",
       "0               Was        -9.937194   Yes                                    \n",
       "1            boozin       -19.579313                                          \n",
       "2               and        -4.195279   Yes                                    \n",
       "3           cruisin       -19.579313                                          \n",
       "4              with        -5.363765   Yes                                    \n",
       "5                my        -5.918125   Yes                                    \n",
       "6            canine       -19.579313                                          \n",
       "7           comrade       -19.579313                                          \n",
       "8                 .        -3.072948                Yes                       \n",
       "9              Went       -19.579313                                          \n",
       "10              all        -5.896713   Yes                                    \n",
       "11              the        -3.425446   Yes                                    \n",
       "12              way        -6.743997                                          \n",
       "13              out        -6.133558   Yes                                    \n",
       "14               to        -3.838520   Yes                                    \n",
       "15           Hudson       -19.579313                                          \n",
       "16              via        -9.864821   Yes                                    \n",
       "17   Brecksville/82       -19.579313                                          \n",
       "18                .        -3.072948                Yes                       \n",
       "19          Looking       -11.468876                                          \n",
       "20              for        -4.913970   Yes                                    \n",
       "21         anything        -7.498493   Yes                                    \n",
       "22               to        -3.838520   Yes                                    \n",
       "23             Yelp       -19.579313                                          \n",
       "24                .        -3.072948                Yes                       \n",
       "25          Nothing        -9.858257   Yes                                    \n",
       "26                .        -3.072948                Yes                       \n",
       "27              Not        -7.758180   Yes                                    \n",
       "28             even        -6.705993   Yes                                    \n",
       "29               in        -4.585874   Yes                                    \n",
       "..              ...              ...   ...          ...         ...     ...   \n",
       "339         dollars        -8.924440                                          \n",
       "340            with        -5.363765   Yes                                    \n",
       "341             the        -3.425446   Yes                                    \n",
       "342            beer        -9.709007                                          \n",
       "343               ,        -3.391480                Yes                       \n",
       "344            just        -5.831590   Yes                                    \n",
       "345              to        -3.838520   Yes                                    \n",
       "346           recap       -19.579313                                          \n",
       "347               .        -3.072948                Yes                       \n",
       "348           Great        -9.451549                                          \n",
       "349           value        -8.531943                                          \n",
       "350               ,        -3.391480                Yes                       \n",
       "351            nice        -8.462502                                          \n",
       "352        location       -10.667636                                          \n",
       "353               ,        -3.391480                Yes                       \n",
       "354             old        -7.795482                                          \n",
       "355             men        -8.724626                                          \n",
       "356               .        -3.072948                Yes                       \n",
       "357               I        -4.064181   Yes                                    \n",
       "358              wo        -8.091730                                          \n",
       "359             n't        -4.883962                                          \n",
       "360            wait        -8.723573                                          \n",
       "361            four        -9.416868   Yes                              Yes   \n",
       "362            more        -6.055923   Yes                                    \n",
       "363           years        -7.156296                                          \n",
       "364              to        -3.838520   Yes                                    \n",
       "365            come        -7.804588                                          \n",
       "366            back        -7.266421   Yes                                    \n",
       "367               .        -3.072948                Yes                       \n",
       "368              \\n        -6.179879                            Yes           \n",
       "\n",
       "    out of vocab.?  \n",
       "0                   \n",
       "1              Yes  \n",
       "2                   \n",
       "3              Yes  \n",
       "4                   \n",
       "5                   \n",
       "6                   \n",
       "7                   \n",
       "8                   \n",
       "9                   \n",
       "10                  \n",
       "11                  \n",
       "12                  \n",
       "13                  \n",
       "14                  \n",
       "15                  \n",
       "16                  \n",
       "17             Yes  \n",
       "18                  \n",
       "19                  \n",
       "20                  \n",
       "21                  \n",
       "22                  \n",
       "23                  \n",
       "24                  \n",
       "25                  \n",
       "26                  \n",
       "27                  \n",
       "28                  \n",
       "29                  \n",
       "..             ...  \n",
       "339                 \n",
       "340                 \n",
       "341                 \n",
       "342                 \n",
       "343                 \n",
       "344                 \n",
       "345                 \n",
       "346                 \n",
       "347                 \n",
       "348                 \n",
       "349                 \n",
       "350                 \n",
       "351                 \n",
       "352                 \n",
       "353                 \n",
       "354                 \n",
       "355                 \n",
       "356                 \n",
       "357                 \n",
       "358                 \n",
       "359                 \n",
       "360                 \n",
       "361                 \n",
       "362                 \n",
       "363                 \n",
       "364                 \n",
       "365                 \n",
       "366                 \n",
       "367                 \n",
       "368                 \n",
       "\n",
       "[369 rows x 7 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_attributes = [(token.orth_,\n",
    "                     token.prob,\n",
    "                     token.is_stop,\n",
    "                     token.is_punct,\n",
    "                     token.is_space,\n",
    "                     token.like_num,\n",
    "                     token.is_oov)\n",
    "                    for token in parsed_review]\n",
    "\n",
    "df = pd.DataFrame(token_attributes,\n",
    "                  columns=['text',\n",
    "                           'log_probability',\n",
    "                           'stop?',\n",
    "                           'punctuation?',\n",
    "                           'whitespace?',\n",
    "                           'number?',\n",
    "                           'out of vocab.?'])\n",
    "\n",
    "df.loc[:, 'stop?':'out of vocab.?'] = (df.loc[:, 'stop?':'out of vocab.?']\n",
    "                                       .applymap(lambda x: u'Yes' if x else u''))\n",
    "                                               \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Phrase Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "_Phrase modeling_ is another approach to learning combinations of tokens that together represent meaningful multi-word concepts. We can develop phrase models by looping over the the words in our reviews and looking for words that _co-occur_ (i.e., appear one after another) together much more frequently than you would expect them to by random chance. The formula our phrase models will use to determine whether two tokens $A$ and $B$ constitute a phrase is:\n",
    "\n",
    "$$\\frac{count(A\\ B) - count_{min}}{count(A) * count(B)} * N > threshold$$\n",
    "\n",
    "...where:\n",
    "* $count(A)$ is the number of times token $A$ appears in the corpus\n",
    "* $count(B)$ is the number of times token $B$ appears in the corpus\n",
    "* $count(A\\ B)$ is the number of times the tokens $A\\ B$ appear in the corpus *in order*\n",
    "* $N$ is the total size of the corpus vocabulary\n",
    "* $count_{min}$ is a user-defined parameter to ensure that accepted phrases occur a minimum number of times\n",
    "* $threshold$ is a user-defined parameter to control how strong of a relationship between two tokens the model requires before accepting them as a phrase\n",
    "\n",
    "Once our phrase model has been trained on our corpus, we can apply it to new text. When our model encounters two tokens in new text that identifies as a phrase, it will merge the two into a single new token.\n",
    "\n",
    "Phrase modeling is superficially similar to named entity detection in that you would expect named entities to become phrases in the model (so _new york_ would become *new\\_york*). But you would also expect multi-word expressions that represent common concepts, but aren't specifically named entities (such as _happy hour_) to also become phrases in the model.\n",
    "\n",
    "We turn to the indispensible [**gensim**](https://radimrehurek.com/gensim/index.html) library to help us with phrase modeling &mdash; the [**Phrases**](https://radimrehurek.com/gensim/models/phrases.html) class in particular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As we're performing phrase modeling, we'll be doing some iterative data transformation at the same time. Our roadmap for data preparation includes:\n",
    "\n",
    "1. Segment text of complete reviews into sentences & normalize text\n",
    "1. First-order phrase modeling $\\rightarrow$ _apply first-order phrase model to transform sentences_\n",
    "1. Second-order phrase modeling $\\rightarrow$ _apply second-order phrase model to transform sentences_\n",
    "1. Apply text normalization and second-order phrase model to text of complete reviews\n",
    "\n",
    "We'll use this transformed data as the input for some higher-level modeling approaches in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "First, let's define a few helper functions that we'll use for text normalization. In particular, the `lemmatized_sentence_corpus` generator function will use spaCy to:\n",
    "- Iterate over the 1M reviews in the `review_txt_all.txt` we created before\n",
    "- Segment the reviews into individual sentences\n",
    "- Remove punctuation and excess whitespace\n",
    "- Lemmatize the text\n",
    "\n",
    "... and do so efficiently in parallel, thanks to spaCy's `nlp.pipe()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def punct_space(token):\n",
    "    \"\"\"\n",
    "    helper function to eliminate tokens\n",
    "    that are pure punctuation or whitespace\n",
    "    \"\"\"\n",
    "    \n",
    "    return token.is_punct or token.is_space\n",
    "\n",
    "def line_review(filename):\n",
    "    \"\"\"\n",
    "    generator function to read in reviews from the file\n",
    "    and un-escape the original line breaks in the text\n",
    "    \"\"\"\n",
    "    \n",
    "    with codecs.open(filename, encoding='utf_8') as f:\n",
    "        for review in f:\n",
    "            yield review.replace('\\\\n', '\\n')\n",
    "            \n",
    "def lemmatized_sentence_corpus(filename):\n",
    "    \"\"\"\n",
    "    generator function to use spaCy to parse reviews,\n",
    "    lemmatize the text, and yield sentences\n",
    "    \"\"\"\n",
    "    \n",
    "    for parsed_review in nlp.pipe(line_review(filename),\n",
    "                                  batch_size=10000, n_threads=4):\n",
    "        \n",
    "        for sent in parsed_review.sents:\n",
    "            yield u' '.join([token.lemma_ for token in sent\n",
    "                             if not punct_space(token)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "unigram_sentences_filepath = os.path.join(intermediate_directory,\n",
    "                                          'unigram_sentences_all.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's use the `lemmatized_sentence_corpus` generator to loop over the original review text, segmenting the reviews into individual sentences and normalizing the text. We'll write this data back out to a new file (`unigram_sentences_all`), with one normalized sentence per line. We'll use this data for learning our phrase models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 µs, sys: 1 µs, total: 7 µs\n",
      "Wall time: 26.7 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if 0 == 1:\n",
    "\n",
    "    with codecs.open(unigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        for sentence in lemmatized_sentence_corpus(review_txt_filepath):\n",
    "            f.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If your data is organized like our `unigram_sentences_all` file now is &mdash; a large text file with one document/sentence per line &mdash; gensim's [**LineSentence**](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.LineSentence) class provides a convenient iterator for working with other gensim components. It *streams* the documents/sentences from disk, so that you never have to hold the entire corpus in RAM at once. This allows you to scale your modeling pipeline up to potentially very large corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "unigram_sentences = LineSentence(unigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3>Let's take a look at a few sample sentences in our new, transformed file.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-PRON- mention that -PRON- be expect a few shell but could not eat through these,,,he say well -PRON- tell -PRON- that -PRON- come with shell and half heartely offer to recook -PRON-\n",
      "\n",
      "this only after the plate have be sit for twenty minute\n",
      "\n",
      "-PRON- avoid -PRON- table like the plague as -PRON- could tell -PRON- could clearly see -PRON- be not eat\n",
      "\n",
      "at the end -PRON- offer a free dessert but by that time -PRON- be a little too late for -PRON-\n",
      "\n",
      "-PRON- do not like to compare high end asian restaurant to pf changs but on this night pf would win\n",
      "\n",
      "try the grand wok in the mgm grand\n",
      "\n",
      "only 4 star because of the expense without -PRON- $ 55 groupon -PRON- meal for two include one large house sake would have be over $ 100\n",
      "\n",
      "however even if -PRON- have not have the groupon -PRON- would have be well worth the money\n",
      "\n",
      "what a find\n",
      "\n",
      "the decor be gorgeous -PRON- be wait on by two attentive garcon sit at the sushi bar so chat with the sushi chef as well and the menu be extremely creative and most importantly delicious\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for unigram_sentence in it.islice(unigram_sentences, 230, 240):\n",
    "    print(u' '.join(unigram_sentence))\n",
    "    print(u'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Next, we'll learn a phrase model that will link individual words into two-word phrases. We'd expect words that together represent a specific concept, like \"`ice cream`\", to be linked together to form a new, single token: \"`ice_cream`\".</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "bigram_model_filepath = os.path.join(intermediate_directory, 'bigram_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35min 34s, sys: 2min 9s, total: 37min 43s\n",
      "Wall time: 6h 11min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute modeling yourself.\n",
    "if 0 == 1:\n",
    "\n",
    "    bigram_model = Phrases(unigram_sentences)\n",
    "\n",
    "    bigram_model.save(bigram_model_filepath)\n",
    "    \n",
    "# load the finished model from disk\n",
    "bigram_model = Phrases.load(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Now that we have a trained phrase model for word pairs, let's apply it to the review sentences data and explore the results.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "bigram_sentences_filepath = os.path.join(intermediate_directory,\n",
    "                                         'bigram_sentences_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gensim/models/phrases.py:274: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48min 1s, sys: 1min 55s, total: 49min 57s\n",
      "Wall time: 5h 20min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if 0 == 1:\n",
    "\n",
    "    with codecs.open(bigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        \n",
    "        for unigram_sentence in unigram_sentences:\n",
    "            \n",
    "            bigram_sentence = u' '.join(bigram_model[unigram_sentence])\n",
    "            \n",
    "            f.write(bigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "bigram_sentences = LineSentence(bigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-PRON- mention that -PRON- be expect a few shell but could not eat through these,,,he say well -PRON- tell -PRON- that -PRON- come with shell and half heartely offer to recook -PRON-\n",
      "\n",
      "this only after the plate have be sit for twenty_minute\n",
      "\n",
      "-PRON- avoid -PRON- table like the plague as -PRON- could tell -PRON- could clearly see -PRON- be not eat\n",
      "\n",
      "at the end -PRON- offer a free dessert but by that time -PRON- be a little too late for -PRON-\n",
      "\n",
      "-PRON- do not like to compare high end asian restaurant to pf_changs but on this night pf would win\n",
      "\n",
      "try the grand_wok in the mgm_grand\n",
      "\n",
      "only 4 star because of the expense without -PRON- $ 55 groupon -PRON- meal for two include one large house sake would have be over $ 100\n",
      "\n",
      "however even if -PRON- have not have the groupon -PRON- would have be well worth the money\n",
      "\n",
      "what a find\n",
      "\n",
      "the decor be gorgeous -PRON- be wait on by two attentive garcon sit at the sushi bar so chat with the sushi chef as well and the menu be extremely creative and most_importantly delicious\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for bigram_sentence in it.islice(bigram_sentences, 230, 240):\n",
    "    print(u' '.join(bigram_sentence))\n",
    "    print(u'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Looks like the phrase modeling worked! We now see two-word phrases, such as \"`twenty_minute`\" and \"`most_importantly`\", linked together in the text as a single token. Next, we'll train a _second-order_ phrase model. We'll apply the second-order phrase model on top of the already-transformed data, so that incomplete word combinations like \"`vanilla_ice cream`\" will become fully joined to \"`vanilla_ice_cream`\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "trigram_model_filepath = os.path.join(intermediate_directory,\n",
    "                                      'trigram_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21min 44s, sys: 4min 16s, total: 26min 1s\n",
      "Wall time: 38min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute modeling yourself.\n",
    "if 0 == 1:\n",
    "\n",
    "    trigram_model = Phrases(bigram_sentences)\n",
    "\n",
    "    trigram_model.save(trigram_model_filepath)\n",
    "    \n",
    "# load the finished model from disk\n",
    "trigram_model = Phrases.load(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3>We'll apply our trained second-order phrase model to our first-order transformed sentences, write the results out to a new file, and explore a few of the second-order transformed sentences.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "trigram_sentences_filepath = os.path.join(intermediate_directory,\n",
    "                                          'trigram_sentences_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gensim/models/phrases.py:274: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38min 36s, sys: 2min 13s, total: 40min 49s\n",
      "Wall time: 47min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if 0 == 1:\n",
    "\n",
    "    with codecs.open(trigram_sentences_filepath, 'w', encoding='utf_8') as f:\n",
    "        \n",
    "        for bigram_sentence in bigram_sentences:\n",
    "            \n",
    "            trigram_sentence = u' '.join(trigram_model[bigram_sentence])\n",
    "            \n",
    "            f.write(trigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "trigram_sentences = LineSentence(trigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-PRON- mention that -PRON- be expect a few shell but could not eat through these,,,he say well -PRON- tell -PRON- that -PRON- come with shell and half heartely offer to recook -PRON-\n",
      "\n",
      "this only after the plate have be sit for twenty_minute\n",
      "\n",
      "-PRON- avoid -PRON- table like the plague as -PRON- could tell -PRON- could clearly see -PRON- be not eat\n",
      "\n",
      "at the end -PRON- offer a free dessert but by that time -PRON- be a little too late for -PRON-\n",
      "\n",
      "-PRON- do not like to compare high end asian restaurant to pf_changs but on this night pf would win\n",
      "\n",
      "try the grand_wok in the mgm_grand\n",
      "\n",
      "only 4 star because of the expense without -PRON- $_55 groupon -PRON- meal for two include one large house sake would have be over $_100\n",
      "\n",
      "however even if -PRON- have not have the groupon -PRON- would have be well worth the money\n",
      "\n",
      "what a find\n",
      "\n",
      "the decor be gorgeous -PRON- be wait on by two attentive garcon sit at the sushi bar so chat with the sushi chef as well and the menu be extremely creative and most_importantly delicious\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for trigram_sentence in it.islice(trigram_sentences, 230, 240):\n",
    "    print(u' '.join(trigram_sentence))\n",
    "    print(u'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Looks like the second-order phrase model was successful. We're now seeing three-word phrases, such as \"`vanilla_ice_cream`\" and \"`cinnamon_ice_cream`\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The final step of our text preparation process circles back to the complete text of the reviews. We're going to run the complete text of the reviews through a pipeline that applies our text normalization and phrase models.\n",
    "\n",
    "In addition, we'll remove stopwords at this point. _Stopwords_ are very common words, like _a_, _the_, _and_, and so on, that serve functional roles in natural language, but typically don't contribute to the overall meaning of text. Filtering stopwords is a common procedure that allows higher-level NLP modeling techniques to focus on the words that carry more semantic weight.\n",
    "\n",
    "Finally, we'll write the transformed text out to a new file, with one review per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "trigram_reviews_filepath = os.path.join(intermediate_directory,\n",
    "                                        'trigram_transformed_reviews_all.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gensim/models/phrases.py:274: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if 0 == 1:\n",
    "\n",
    "    with codecs.open(trigram_reviews_filepath, 'w', encoding='utf_8') as f:\n",
    "        \n",
    "        for parsed_review in nlp.pipe(line_review(review_txt_filepath),\n",
    "                                      batch_size=10000, n_threads=4):\n",
    "            \n",
    "            # lemmatize the text, removing punctuation and whitespace\n",
    "            unigram_review = [token.lemma_ for token in parsed_review\n",
    "                              if not punct_space(token)]\n",
    "            \n",
    "            # apply the first-order and second-order phrase models\n",
    "            bigram_review = bigram_model[unigram_review]\n",
    "            trigram_review = trigram_model[bigram_review]\n",
    "            \n",
    "            # remove any remaining stopwords\n",
    "            trigram_review = [term for term in trigram_review\n",
    "                              if term not in spacy.en.language_data.STOP_WORDS]\n",
    "            \n",
    "            # write the transformed review as a line in the new file\n",
    "            trigram_review = u' '.join(trigram_review)\n",
    "            f.write(trigram_review + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let's preview the results. We'll grab one review from the file with the original, untransformed text, grab the same review from the file with the normalized and transformed text, and compare the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print(u'Original:' + u'\\n')\n",
    "\n",
    "for review in it.islice(line_review(review_txt_filepath), 11, 12):\n",
    "    print review\n",
    "\n",
    "print(u'----' + u'\\n')\n",
    "print(u'Transformed:' + u'\\n')\n",
    "\n",
    "with codecs.open(trigram_reviews_filepath, encoding='utf_8') as f:\n",
    "    for review in it.islice(f, 11, 12):\n",
    "        print review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>Most of the grammatical structure has been scrubbed from the text &mdash; capitalization, articles/conjunctions, punctuation, spacing, etc. However, much of the general semantic *meaning* is still present. Also, multi-word concepts such as \"`friday_night`\" and \"`above_average`\" have been joined into single tokens. The review text is now ready for higher-level modeling.</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Topic Modeling with Latent Dirichlet Allocation (_LDA_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*Topic modeling* is family of techniques that can be used to describe and summarize the documents in a corpus according to a set of latent \"topics\". For this demo, we'll be using [*Latent Dirichlet Allocation*](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf) or LDA, a popular approach to topic modeling.\n",
    "\n",
    "In many conventional NLP applications, documents are represented a mixture of the individual tokens (words and phrases) they contain. In other words, a document is represented as a *vector* of token counts. There are two layers in this model &mdash; documents and tokens &mdash; and the size or dimensionality of the document vectors is the number of tokens in the corpus vocabulary. This approach has a number of disadvantages:\n",
    "* Document vectors tend to be large (one dimension for each token $\\Rightarrow$ lots of dimensions)\n",
    "* They also tend to be very sparse. Any given document only contains a small fraction of all tokens in the vocabulary, so most values in the document's token vector are 0.\n",
    "* The dimensions are fully indepedent from each other &mdash; there's no sense of connection between related tokens, such as _knife_ and _fork_.\n",
    "\n",
    "LDA injects a third layer into this conceptual model. Documents are represented as a mixture of a pre-defined number of *topics*, and the *topics* are represented as a mixture of the individual tokens in the vocabulary. The number of topics is a model hyperparameter selected by the practitioner. LDA makes a prior assumption that the (document, topic) and (topic, token) mixtures follow [*Dirichlet*](https://en.wikipedia.org/wiki/Dirichlet_distribution) probability distributions. This assumption encourages documents to consist mostly of a handful of topics, and topics to consist mostly of a modest set of the tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![LDA](https://s3.amazonaws.com/skipgram-images/LDA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "LDA is fully unsupervised. The topics are \"discovered\" automatically from the data by trying to maximize the likelihood of observing the documents in your corpus, given the modeling assumptions. They are expected to capture some latent structure and organization within the documents, and often have a meaningful human interpretation for people familiar with the subject material.\n",
    "\n",
    "We'll again turn to gensim to assist with data preparation and modeling. In particular, gensim offers a high-performance parallelized implementation of LDA with its [**LdaMulticore**](https://radimrehurek.com/gensim/models/ldamulticore.html) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import warnings\n",
    "import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<h3>The first step to creating an LDA model is to learn the full vocabulary of the corpus to be modeled. We'll use gensim's [**Dictionary**](https://radimrehurek.com/gensim/corpora/dictionary.html) class for this.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "trigram_dictionary_filepath = os.path.join(intermediate_directory,\n",
    "                                           'trigram_dict_all.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to learn the dictionary yourself.\n",
    "if 0 == 1:\n",
    "\n",
    "    trigram_reviews = LineSentence(trigram_reviews_filepath)\n",
    "\n",
    "    # learn the dictionary by iterating over all of the reviews\n",
    "    trigram_dictionary = Dictionary(trigram_reviews)\n",
    "    \n",
    "    # filter tokens that are very rare or too common from\n",
    "    # the dictionary (filter_extremes) and reassign integer ids (compactify)\n",
    "    trigram_dictionary.filter_extremes(no_below=10, no_above=0.4)\n",
    "    trigram_dictionary.compactify()\n",
    "\n",
    "    trigram_dictionary.save(trigram_dictionary_filepath)\n",
    "    \n",
    "# load the finished dictionary from disk\n",
    "trigram_dictionary = Dictionary.load(trigram_dictionary_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Like many NLP techniques, LDA uses a simplifying assumption known as the [*bag-of-words* model](https://en.wikipedia.org/wiki/Bag-of-words_model). In the bag-of-words model, a document is represented by the counts of distinct terms that occur within it. Additional information, such as word order, is discarded. \n",
    "\n",
    "Using the gensim Dictionary we learned to generate a bag-of-words representation for each review. The `trigram_bow_generator` function implements this. We'll save the resulting bag-of-words reviews as a matrix.\n",
    "\n",
    "In the following code, \"bag-of-words\" is abbreviated as `bow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "trigram_bow_filepath = os.path.join(intermediate_directory,\n",
    "                                    'trigram_bow_corpus_all.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def trigram_bow_generator(filepath):\n",
    "    \"\"\"\n",
    "    generator function to read reviews from a file\n",
    "    and yield a bag-of-words representation\n",
    "    \"\"\"\n",
    "    \n",
    "    for review in LineSentence(filepath):\n",
    "        yield trigram_dictionary.doc2bow(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to build the bag-of-words corpus yourself.\n",
    "if 0 == 1:\n",
    "\n",
    "    # generate bag-of-words representations for\n",
    "    # all reviews and save them as a matrix\n",
    "    MmCorpus.serialize(trigram_bow_filepath,\n",
    "                       trigram_bow_generator(trigram_reviews_filepath))\n",
    "    \n",
    "# load the finished bag-of-words corpus from disk\n",
    "trigram_bow_corpus = MmCorpus(trigram_bow_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3>With the bag-of-words corpus, we're finally ready to learn our topic model from the reviews. We simply need to pass the bag-of-words matrix and Dictionary from our previous steps to `LdaMulticore` as inputs, along with the number of topics the model should learn. For this demo, we're asking for 50 topics.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lda_model_filepath = os.path.join(intermediate_directory, 'lda_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to train the LDA model yourself.\n",
    "if 0 == 1:\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        \n",
    "        # workers => sets the parallelism, and should be\n",
    "        # set to your number of physical cores minus one\n",
    "        lda = LdaMulticore(trigram_bow_corpus,\n",
    "                           num_topics=50,\n",
    "                           id2word=trigram_dictionary,\n",
    "                           workers=3)\n",
    "    \n",
    "    lda.save(lda_model_filepath)\n",
    "    \n",
    "# load the finished LDA model from disk\n",
    "lda = LdaMulticore.load(lda_model_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Our topic model is now trained and ready to use! Since each topic is represented as a mixture of tokens, you can manually inspect which tokens have been grouped together into which topics to try to understand the patterns the model has discovered in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def explore_topic(topic_number, topn=25):\n",
    "    \"\"\"\n",
    "    accept a user-supplied topic number and\n",
    "    print out a formatted list of the top terms\n",
    "    \"\"\"\n",
    "        \n",
    "    print u'{:20} {}'.format(u'term', u'frequency') + u'\\n'\n",
    "\n",
    "    for term, frequency in lda.show_topic(topic_number, topn=25):\n",
    "        print u'{:20} {:.3f}'.format(term, round(frequency, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "explore_topic(topic_number=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The first topic has strong associations with words like *taco*, *salsa*, *chip*, *burrito*, and *margarita*, as well as a handful of more general words. You might call this the **Mexican food** topic!\n",
    "\n",
    "It's possible to go through and inspect each topic in the same way, and try to assign a human-interpretable label that captures the essence of each one. I've given it a shot for all 50 topics below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "topic_names = {0: u'mexican',\n",
    "               1: u'menu',\n",
    "               2: u'thai',\n",
    "               3: u'steak',\n",
    "               4: u'donuts & appetizers',\n",
    "               5: u'specials',\n",
    "               6: u'soup',\n",
    "               7: u'wings, sports bar',\n",
    "               8: u'foreign language',\n",
    "               9: u'las vegas',\n",
    "               10: u'chicken',\n",
    "               11: u'aria buffet',\n",
    "               12: u'noodles',\n",
    "               13: u'ambience & seating',\n",
    "               14: u'sushi',\n",
    "               15: u'arizona',\n",
    "               16: u'family',\n",
    "               17: u'price',\n",
    "               18: u'sweet',\n",
    "               19: u'waiting',\n",
    "               20: u'general',\n",
    "               21: u'tapas',\n",
    "               22: u'dirty',\n",
    "               23: u'customer service',\n",
    "               24: u'restrooms',\n",
    "               25: u'chinese',\n",
    "               26: u'gluten free',\n",
    "               27: u'pizza',\n",
    "               28: u'seafood',\n",
    "               29: u'amazing',\n",
    "               30: u'eat, like, know, want',\n",
    "               31: u'bars',\n",
    "               32: u'breakfast',\n",
    "               33: u'location & time',\n",
    "               34: u'italian',\n",
    "               35: u'barbecue',\n",
    "               36: u'arizona',\n",
    "               37: u'indian',\n",
    "               38: u'latin & cajun',\n",
    "               39: u'burger & fries',\n",
    "               40: u'vegetarian',\n",
    "               41: u'lunch buffet',\n",
    "               42: u'customer service',\n",
    "               43: u'taco, ice cream',\n",
    "               44: u'high cuisine',\n",
    "               45: u'healthy',\n",
    "               46: u'salad & sandwich',\n",
    "               47: u'greek',\n",
    "               48: u'poor experience',\n",
    "               49: u'wine & dine'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "topic_names_filepath = os.path.join(intermediate_directory, 'topic_names.pkl')\n",
    "\n",
    "with open(topic_names_filepath, 'w') as f:\n",
    "    pickle.dump(topic_names, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You can see that, along with **mexican**, there are a variety of topics related to different styles of food, such as **thai**, **steak**, **sushi**, **pizza**, and so on. In addition, there are topics that are more related to the overall restaurant *experience*, like **ambience & seating**, **good service**, **waiting**, and **price**.\n",
    "\n",
    "Beyond these two categories, there are still some topics that are difficult to apply a meaningful human interpretation to, such as topic 30 and 43.\n",
    "\n",
    "Manually reviewing the top terms for each topic is a helpful exercise, but to get a deeper understanding of the topics and how they relate to each other, we need to visualize the data &mdash; preferably in an interactive format. Fortunately, we have the fantastic [**pyLDAvis**](https://pyldavis.readthedocs.io/en/latest/readme.html) library to help with that!\n",
    "\n",
    "pyLDAvis includes a one-line function to take topic models created with gensim and prepare their data for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "LDAvis_data_filepath = os.path.join(intermediate_directory, 'ldavis_prepared')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if 0 == 1:\n",
    "\n",
    "    LDAvis_prepared = pyLDAvis.gensim.prepare(lda, trigram_bow_corpus,\n",
    "                                              trigram_dictionary)\n",
    "\n",
    "    with open(LDAvis_data_filepath, 'w') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "        \n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath) as f:\n",
    "    LDAvis_prepared = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`pyLDAvis.display(...)` displays the topic model visualization in-line in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pyLDAvis.display(LDAvis_prepared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Wait, what am I looking at again?\n",
    "There are a lot of moving parts in the visualization. Here's a brief summary:\n",
    "\n",
    "* On the left, there is a plot of the \"distance\" between all of the topics (labeled as the _Intertopic Distance Map_)\n",
    "  * The plot is rendered in two dimensions according a [*multidimensional scaling (MDS)*](https://en.wikipedia.org/wiki/Multidimensional_scaling) algorithm. Topics that are generally similar should be appear close together on the plot, while *dis*similar topics should appear far apart.\n",
    "  * The relative size of a topic's circle in the plot corresponds to the relative frequency of the topic in the corpus.\n",
    "  * An individual topic may be selected for closer scrutiny by clicking on its circle, or entering its number in the \"selected topic\" box in the upper-left.\n",
    "* On the right, there is a bar chart showing top terms.\n",
    "  * When no topic is selected in the plot on the left, the bar chart shows the top-30 most \"salient\" terms in the corpus. A term's *saliency* is a measure of both how frequent the term is in the corpus and how \"distinctive\" it is in distinguishing between different topics.\n",
    "  * When a particular topic is selected, the bar chart changes to show the top-30 most \"relevant\" terms for the selected topic. The relevance metric is controlled by the parameter $\\lambda$, which can be adjusted with a slider above the bar chart.\n",
    "    * Setting the $\\lambda$ parameter close to 1.0 (the default) will rank the terms solely according to their probability within the topic.\n",
    "    * Setting $\\lambda$ close to 0.0 will rank the terms solely according to their \"distinctiveness\" or \"exclusivity\" within the topic &mdash; i.e., terms that occur *only* in this topic, and do not occur in other topics.\n",
    "    * Setting $\\lambda$ to values between 0.0 and 1.0 will result in an intermediate ranking, weighting term probability and exclusivity accordingly.\n",
    "* Rolling the mouse over a term in the bar chart on the right will cause the topic circles to resize in the plot on the left, to show the strength of the relationship between the topics and the selected term.\n",
    "\n",
    "A more detailed explanation of the pyLDAvis visualization can be found [here](https://cran.r-project.org/web/packages/LDAvis/vignettes/details.pdf). Unfortunately, though the data used by gensim and pyLDAvis are the same, they don't use the same ID numbers for topics. If you need to match up topics in gensim's `LdaMulticore` object and pyLDAvis' visualization, you have to dig through the terms manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Analyzing our LDA model\n",
    "The interactive visualization pyLDAvis produces is helpful for both:\n",
    "1. Better understanding and interpreting individual topics, and\n",
    "1. Better understanding the relationships between the topics.\n",
    "\n",
    "For (1), you can manually select each topic to view its top most freqeuent and/or \"relevant\" terms, using different values of the $\\lambda$ parameter. This can help when you're trying to assign a human interpretable name or \"meaning\" to each topic.\n",
    "\n",
    "For (2), exploring the _Intertopic Distance Plot_ can help you learn about how topics relate to each other, including potential higher-level structure between groups of topics.\n",
    "\n",
    "In our plot, there is a stark divide along the x-axis, with two topics far to the left and most of the remaining 48 far to the right. Inspecting the two outlier topics provides a plausible explanation: both topics contain many non-English words, while most of the rest of the topics are in English. So, one of the main attributes that distinguish the reviews in the dataset from one another is their language.\n",
    "\n",
    "This finding isn't entirely a surprise. In addition to English-speaking cities, the Yelp dataset includes reviews of businesses in Montreal and Karlsruhe, Germany, often written in French and German, respectively. Multiple languages isn't a problem for our demo, but for a real NLP application, you might need to ensure that the text you're processing is written in English (or is at least tagged for language) before passing it along to some downstream processing. If that were the case, the divide along the x-axis in the topic plot would immediately alert you to a potential data quality issue.\n",
    "\n",
    "The y-axis separates two large groups of topics &mdash; let's call them \"super-topics\" &mdash; one in the upper-right quadrant and the other in the lower-right quadrant. These super-topics correlate reasonably well with the pattern we'd noticed while naming the topics:\n",
    "* The super-topic in the *lower*-right tends to be about *food*. It groups together the **burger & fries**, **breakfast**, **sushi**, **barbecue**, and **greek** topics, among others.\n",
    "* The super-topic in the *upper*-right tends to be about other elements of the *restaurant experience*. It groups together the **ambience & seating**, **location & time**, **family**, and **customer service** topics, among others.\n",
    "\n",
    "So, in addition to the 50 direct topics the model has learned, our analysis suggests a higher-level pattern in the data. Restaurant reviewers in the Yelp dataset talk about two main things in their reviews, in general: (1) the food, and (2) their overall restaurant experience. For this dataset, this is a very intuitive result, and we probably didn't need a sophisticated modeling technique to tell it to us. When working with datasets from other domains, though, such high-level patterns may be much less obvious from the outset &mdash; and that's where topic modeling can help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Describing text with LDA\n",
    "Beyond data exploration, one of the key uses for an LDA model is providing a compact, quantitative description of natural language text. Once an LDA model has been trained, it can be used to represent free text as a mixture of the topics the model learned from the original corpus. This mixture can be interpreted as a probability distribution across the topics, so the LDA representation of a paragraph of text might look like 50% _Topic A_, 20% _Topic B_, 20% _Topic C_, and 10% _Topic D_.\n",
    "\n",
    "To use an LDA model to generate a vector representation of new text, you'll need to apply any text preprocessing steps you used on the model's training corpus to the new text, too. For our model, the preprocessing steps we used include:\n",
    "1. Using spaCy to remove punctuation and lemmatize the text\n",
    "1. Applying our first-order phrase model to join word pairs\n",
    "1. Applying our second-order phrase model to join longer phrases\n",
    "1. Removing stopwords\n",
    "1. Creating a bag-of-words representation\n",
    "\n",
    "Once you've applied these preprocessing steps to the new text, it's ready to pass directly to the model to create an LDA representation. The `lda_description(...)` function will perform all these steps for us, including printing the resulting topical description of the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_sample_review(review_number):\n",
    "    \"\"\"\n",
    "    retrieve a particular review index\n",
    "    from the reviews file and return it\n",
    "    \"\"\"\n",
    "    \n",
    "    return list(it.islice(line_review(review_txt_filepath),\n",
    "                          review_number, review_number+1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def lda_description(review_text, min_topic_freq=0.05):\n",
    "    \"\"\"\n",
    "    accept the original text of a review and (1) parse it with spaCy,\n",
    "    (2) apply text pre-proccessing steps, (3) create a bag-of-words\n",
    "    representation, (4) create an LDA representation, and\n",
    "    (5) print a sorted list of the top topics in the LDA representation\n",
    "    \"\"\"\n",
    "    \n",
    "    # parse the review text with spaCy\n",
    "    parsed_review = nlp(review_text)\n",
    "    \n",
    "    # lemmatize the text and remove punctuation and whitespace\n",
    "    unigram_review = [token.lemma_ for token in parsed_review\n",
    "                      if not punct_space(token)]\n",
    "    \n",
    "    # apply the first-order and secord-order phrase models\n",
    "    bigram_review = bigram_model[unigram_review]\n",
    "    trigram_review = trigram_model[bigram_review]\n",
    "    \n",
    "    # remove any remaining stopwords\n",
    "    trigram_review = [term for term in trigram_review\n",
    "                      if not term in spacy.en.STOPWORDS]\n",
    "    \n",
    "    # create a bag-of-words representation\n",
    "    review_bow = trigram_dictionary.doc2bow(trigram_review)\n",
    "    \n",
    "    # create an LDA representation\n",
    "    review_lda = lda[review_bow]\n",
    "    \n",
    "    # sort with the most highly related topics first\n",
    "    review_lda = sorted(review_lda, key=lambda (topic_number, freq): -freq)\n",
    "    \n",
    "    for topic_number, freq in review_lda:\n",
    "        if freq < min_topic_freq:\n",
    "            break\n",
    "            \n",
    "        # print the most highly related topic names and frequencies\n",
    "        print '{:25} {}'.format(topic_names[topic_number],\n",
    "                                round(freq, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_review = get_sample_review(50)\n",
    "print sample_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lda_description(sample_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sample_review = get_sample_review(100)\n",
    "print sample_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lda_description(sample_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word Vector Embedding with Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Pop quiz! Can you complete this text snippet?\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![word2vec quiz](https://s3.amazonaws.com/skipgram-images/word2vec-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<br><br><br>\n",
    "You just demonstrated the core machine learning concept behind word vector embedding models!\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![word2vec quiz 2](https://s3.amazonaws.com/skipgram-images/word2vec-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The goal of *word vector embedding models*, or *word vector models* for short, is to learn dense, numerical vector representations for each term in a corpus vocabulary. If the model is successful, the vectors it learns about each term should encode some information about the *meaning* or *concept* the term represents, and the relationship between it and other terms in the vocabulary. Word vector models are also fully unsupervised &mdash they learn all of these meanings and relationships solely by analyzing the text of the corpus, without any advance knowledge provided.\n",
    "\n",
    "Perhaps the best-known word vector model is [word2vec](https://arxiv.org/pdf/1301.3781v3.pdf), originally proposed in 2013. The general idea of word2vec is, for a given *focus word*, to use the *context* of the word &mdash; i.e., the other words immediately before and after it &mdash; to provide hints about what the focus word might mean. To do this, word2vec uses a *sliding window* technique, where it considers snippets of text only a few tokens long at a time.\n",
    "\n",
    "At the start of the learning process, the model initializes random vectors for all terms in the corpus vocabulary. The model then slides the window across every snippet of text in the corpus, with each word taking turns as the focus word. Each time the model considers a new snippet, it tries to learn some information about the focus word based on the surrouding context, and it \"nudges\" the words' vector representations accordingly. One complete pass sliding the window across all of the corpus text is known as a training *epoch*. It's common to train a word2vec model for multiple passes/epochs over the corpus. Over time, the model rearranges the terms' vector representations such that terms that frequently appear in similar contexts have vector representations that are *close* to each other in vector space.\n",
    "\n",
    "For a deeper dive into word2vec's machine learning process, see [here](https://arxiv.org/pdf/1411.2738v4.pdf).\n",
    "\n",
    "Word2vec has a number of user-defined hyperparameters, including:\n",
    "- The dimensionality of the vectors. Typical choices include a few dozen to several hundred.\n",
    "- The width of the sliding window, in tokens. Five is a common default choice, but narrower and wider windows are possible.\n",
    "- The number of training epochs.\n",
    "\n",
    "For using word2vec in Python, [gensim](https://rare-technologies.com/deep-learning-with-word2vec-and-gensim/) comes to the rescue again! It offers a [highly-optimized](https://rare-technologies.com/word2vec-in-python-part-two-optimizing/), [parallelized](https://rare-technologies.com/parallelizing-word2vec-in-python/) implementation of the word2vec algorithm with its [Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "trigram_sentences = LineSentence(trigram_sentences_filepath)\n",
    "word2vec_filepath = os.path.join(intermediate_directory, 'word2vec_model_all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We'll train our word2vec model using the normalized sentences with our phrase models applied. We'll use 100-dimensional vectors, and set up our training process to run for twelve epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to train the word2vec model yourself.\n",
    "if 0 == 1:\n",
    "\n",
    "    # initiate the model and perform the first epoch of training\n",
    "    food2vec = Word2Vec(trigram_sentences, size=100, window=5,\n",
    "                        min_count=20, sg=1, workers=4)\n",
    "    \n",
    "    food2vec.save(word2vec_filepath)\n",
    "\n",
    "    # perform another 11 epochs of training\n",
    "    for i in range(1,12):\n",
    "\n",
    "        food2vec.train(trigram_sentences)\n",
    "        food2vec.save(word2vec_filepath)\n",
    "        \n",
    "# load the finished model from disk\n",
    "food2vec = Word2Vec.load(word2vec_filepath)\n",
    "food2vec.init_sims()\n",
    "\n",
    "print u'{} training epochs so far.'.format(food2vec.train_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "On my four-core machine, each epoch over all the text in the ~1 million Yelp reviews takes about 5-10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print u'{:,} terms in the food2vec vocabulary.'.format(len(food2vec.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's take a peek at the word vectors our model has learned. We'll create a pandas DataFrame with the terms as the row labels, and the 100 dimensions of the word vector model as the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# build a list of the terms, integer indices,\n",
    "# and term counts from the food2vec model vocabulary\n",
    "ordered_vocab = [(term, voc.index, voc.count)\n",
    "                 for term, voc in food2vec.vocab.iteritems()]\n",
    "\n",
    "# sort by the term counts, so the most common terms appear first\n",
    "ordered_vocab = sorted(ordered_vocab, key=lambda (term, index, count): -count)\n",
    "\n",
    "# unzip the terms, integer indices, and counts into separate lists\n",
    "ordered_terms, term_indices, term_counts = zip(*ordered_vocab)\n",
    "\n",
    "# create a DataFrame with the food2vec vectors as data,\n",
    "# and the terms as row labels\n",
    "word_vectors = pd.DataFrame(food2vec.syn0norm[term_indices, :],\n",
    "                            index=ordered_terms)\n",
    "\n",
    "word_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Holy wall of numbers! This DataFrame has 50,835 rows &mdash; one for each term in the vocabulary &mdash; and 100 colums. Our model has learned a quantitative vector representation for each term, as expected.\n",
    "\n",
    "Put another way, our model has \"embedded\" the terms into a 100-dimensional vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### So... what can we do with all these numbers?\n",
    "The first thing we can use them for is to simply look up related words and phrases for a given term of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_related_terms(token, topn=10):\n",
    "    \"\"\"\n",
    "    look up the topn most similar terms to token\n",
    "    and print them as a formatted list\n",
    "    \"\"\"\n",
    "\n",
    "    for word, similarity in food2vec.most_similar(positive=[token], topn=topn):\n",
    "\n",
    "        print u'{:20} {}'.format(word, round(similarity, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### What things are like Burger King?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "get_related_terms(u'burger_king')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The model has learned that fast food restaurants are similar to each other! In particular, *mcdonalds* and *wendy's* are the most similar to Burger King, according to this dataset. In addition, the model has found that alternate spellings for the same entities are probably related, such as *mcdonalds*, *mcdonald's* and *mcd's*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### When is happy hour?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "get_related_terms(u'happy_hour')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The model has noticed several alternate spellings for happy hour, such as *hh* and *happy hr*, and assesses them as highly related. If you were looking for reviews about happy hour, such alternate spellings would be very helpful to know.\n",
    "\n",
    "Taking a deeper look &mdash; the model has turned up phrases like *3-6pm*, *4-7pm*, and *mon-fri*, too. This is especially interesting, because the model has no advance knowledge at all about what happy hour is, and what time of day it should be. But simply by scanning through restaurant reviews, the model has discovered that the concept of happy hour has something very important to do with that block of time around 3-7pm on weekdays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Let's make pasta tonight. Which style do you want?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "get_related_terms(u'pasta', topn=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Word algebra!\n",
    "No self-respecting word2vec demo would be complete without a healthy dose of *word algebra*, also known as *analogy completion*.\n",
    "\n",
    "The core idea is that once words are represented as numerical vectors, you can do math with them. The mathematical procedure goes like this:\n",
    "1. Provide a set of words or phrases that you'd like to add or subtract.\n",
    "1. Look up the vectors that represent those terms in the word vector model.\n",
    "1. Add and subtract those vectors to produce a new, combined vector.\n",
    "1. Look up the most similar vector(s) to this new, combined vector via cosine similarity.\n",
    "1. Return the word(s) associated with the similar vector(s).\n",
    "\n",
    "But more generally, you can think of the vectors that represent each word as encoding some information about the *meaning* or *concepts* of the word. What happens when you ask the model to combine the meaning and concepts of words in new ways? Let's see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def word_algebra(add=[], subtract=[], topn=1):\n",
    "    \"\"\"\n",
    "    combine the vectors associated with the words provided\n",
    "    in add= and subtract=, look up the topn most similar\n",
    "    terms to the combined vector, and print the result(s)\n",
    "    \"\"\"\n",
    "    answers = food2vec.most_similar(positive=add, negative=subtract, topn=topn)\n",
    "    \n",
    "    for term, similarity in answers:\n",
    "        print term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### breakfast + lunch = ?\n",
    "Let's start with a softball."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word_algebra(add=[u'breakfast', u'lunch'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "OK, so the model knows that *brunch* is a combination of *breakfast* and *lunch*. What else?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### lunch - day + night = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word_algebra(add=[u'lunch', u'night'], subtract=[u'day'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we're getting a bit more nuanced. The model has discovered that:\n",
    "- Both *lunch* and *dinner* are meals\n",
    "- The main difference between them is time of day\n",
    "- Day and night are times of day\n",
    "- Lunch is associated with day, and dinner is associated with night\n",
    "\n",
    "What else?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### taco - mexican + chinese = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word_algebra(add=[u'taco', u'chinese'], subtract=[u'mexican'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Here's an entirely new and different type of relationship that the model has learned.\n",
    "- It knows that tacos are a characteristic example of Mexican food\n",
    "- It knows that Mexican and Chinese are both styles of food\n",
    "- If you subtract *Mexican* from *taco*, you're left with something like the concept of a *\"characteristic type of food\"*, which is represented as a new vector\n",
    "- If you add that new *\"characteristic type of food\"* vector to Chinese, you get *dumpling*.\n",
    "\n",
    "What else?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### bun - american + mexican = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word_algebra(add=[u'bun', u'mexican'], subtract=[u'american'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The model knows that both *buns* and *tortillas* are the doughy thing that goes on the outside of your real food, and that the primary difference between them is the style of food they're associated with.\n",
    "\n",
    "What else?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### filet mignon - beef + seafood = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word_algebra(add=[u'filet_mignon', u'seafood'], subtract=[u'beef'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The model has learned a concept of *delicacy*. If you take filet mignon and subtract beef from it, you're left with a vector that roughly corresponds to delicacy. If you add the delicacy vector to *seafood*, you get *raw oyster*.\n",
    "\n",
    "What else?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### coffee - drink + snack = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word_algebra(add=[u'coffee', u'snack'], subtract=[u'drink'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The model knows that if you're on your coffee break, but instead of drinking something, you're eating something... that thing is most likely a pastry.\n",
    "\n",
    "What else?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Burger King + fine dining = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word_algebra(add=[u'burger_king', u'fine_dining'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Touché. It makes sense, though. The model has learned that both Burger King and Denny's are large chains, and that both serve fast, casual, American-style food. But Denny's has some elements that are slightly more upscale, such as printed menus and table service. Fine dining, indeed.\n",
    "\n",
    "*What if we keep going?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Denny's + fine dining = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word_algebra(add=[u\"denny_'s\", u'fine_dining'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This seems like a good place to land... what if we explore the vector space around *Applebee's* a bit, in a few different directions? Let's see what we find.\n",
    "\n",
    "#### Applebee's + italian = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_algebra(add=[u\"applebee_'s\", u'italian'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Applebee's + pancakes = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word_algebra(add=[u\"applebee_'s\", u'pancakes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Applebee's + pizza = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word_algebra(add=[u\"applebee_'s\", u'pizza'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You could do this all day. One last analogy before we move on..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### wine - grapes + barley = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word_algebra(add=[u'wine', u'barley'], subtract=[u'grapes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Word Vector Visualization with t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "[t-Distributed Stochastic Neighbor Embedding](https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf), or *t-SNE* for short, is a dimensionality reduction technique to assist with visualizing high-dimensional datasets. It attempts to map high-dimensional data onto a low two- or three-dimensional representation such that the relative distances between points are preserved as closely as possible in both high-dimensional and low-dimensional space.\n",
    "\n",
    "scikit-learn provides a convenient implementation of the t-SNE algorithm with its [TSNE](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Our input for t-SNE will be the DataFrame of word vectors we created before. Let's first:\n",
    "1. Drop stopwords &mdash; it's probably not too interesting to visualize *the*, *of*, *or*, and so on\n",
    "1. Take only the 5,000 most frequent terms in the vocabulary &mdash; no need to visualize all ~50,000 terms right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tsne_input = word_vectors.drop(spacy.en.STOPWORDS, errors=u'ignore')\n",
    "tsne_input = tsne_input.head(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tsne_input.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tsne_filepath = os.path.join(intermediate_directory,\n",
    "                             u'tsne_model')\n",
    "\n",
    "tsne_vectors_filepath = os.path.join(intermediate_directory,\n",
    "                                     u'tsne_vectors.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if 0 == 1:\n",
    "    \n",
    "    tsne = TSNE()\n",
    "    tsne_vectors = tsne.fit_transform(tsne_input.values)\n",
    "    \n",
    "    with open(tsne_filepath, 'w') as f:\n",
    "        pickle.dump(tsne, f)\n",
    "\n",
    "    pd.np.save(tsne_vectors_filepath, tsne_vectors)\n",
    "    \n",
    "with open(tsne_filepath) as f:\n",
    "    tsne = pickle.load(f)\n",
    "    \n",
    "tsne_vectors = pd.np.load(tsne_vectors_filepath)\n",
    "\n",
    "tsne_vectors = pd.DataFrame(tsne_vectors,\n",
    "                            index=pd.Index(tsne_input.index),\n",
    "                            columns=[u'x_coord', u'y_coord'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we have a two-dimensional representation of our data! Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tsne_vectors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tsne_vectors[u'word'] = tsne_vectors.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Plotting with Bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import HoverTool, ColumnDataSource, value\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# add our DataFrame as a ColumnDataSource for Bokeh\n",
    "plot_data = ColumnDataSource(tsne_vectors)\n",
    "\n",
    "# create the plot and configure the\n",
    "# title, dimensions, and tools\n",
    "tsne_plot = figure(title=u't-SNE Word Embeddings',\n",
    "                   plot_width = 800,\n",
    "                   plot_height = 800,\n",
    "                   tools= (u'pan, wheel_zoom, box_zoom,'\n",
    "                           u'box_select, resize, reset'),\n",
    "                   active_scroll=u'wheel_zoom')\n",
    "\n",
    "# add a hover tool to display words on roll-over\n",
    "tsne_plot.add_tools( HoverTool(tooltips = u'@word') )\n",
    "\n",
    "# draw the words as circles on the plot\n",
    "tsne_plot.circle(u'x_coord', u'y_coord', source=plot_data,\n",
    "                 color=u'blue', line_alpha=0.2, fill_alpha=0.1,\n",
    "                 size=10, hover_line_color=u'black')\n",
    "\n",
    "# configure visual elements of the plot\n",
    "tsne_plot.title.text_font_size = value(u'16pt')\n",
    "tsne_plot.xaxis.visible = False\n",
    "tsne_plot.yaxis.visible = False\n",
    "tsne_plot.grid.grid_line_color = None\n",
    "tsne_plot.outline_line_color = None\n",
    "\n",
    "# engage!\n",
    "show(tsne_plot);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Whew! Let's round up the major components that we've seen:\n",
    "1. Text processing with **spaCy**\n",
    "1. Automated **phrase modeling**\n",
    "1. Topic modeling with **LDA** $\\ \\longrightarrow\\ $ visualization with **pyLDAvis**\n",
    "1. Word vector modeling with **word2vec** $\\ \\longrightarrow\\ $ visualization with **t-SNE**\n",
    "\n",
    "#### Why use these models?\n",
    "Dense vector representations for text like LDA and word2vec can greatly improve performance for a number of common, text-heavy problems like:\n",
    "- Text classification\n",
    "- Search\n",
    "- Recommendations\n",
    "- Question answering\n",
    "\n",
    "...and more generally are a powerful way machines can help humans make sense of what's in a giant pile of text. They're also often useful as a pre-processing step for many other downstream machine learning applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data Science @ S&P Global &mdash; *we are hiring!*"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
